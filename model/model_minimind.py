# üìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìò
#                                             MiniMind Config
# üìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìò

'''
PretrainedConfig ÊòØÊâÄÊúâÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÈÖçÁΩÆÁ±ªÁöÑÁà∂Á±ªÔºåÊèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑÊé•Âè£ÔºàÂ¶Ç‰øùÂ≠ò / Âä†ËΩΩÈÖçÁΩÆ„ÄÅÈ™åËØÅÂèÇÊï∞ÂêàÊ≥ïÊÄßÁ≠âÔºâÔºå
Ëá™ÂÆö‰πâÊ®°ÂûãÊó∂ÈúÄÁªßÊâøÊ≠§Á±ª‰ª•ÂÖºÂÆπ transformers ÁîüÊÄÅÔºàÂ¶Ç AutoConfig Ëá™Âä®Âä†ËΩΩÔºâ
'''
from transformers import PretrainedConfig

'''
1,Âú®ÂΩìÂâçÂ∑≤ÊúâÁöÑÊâÄÊúâËØçÔºàËØ≠‰πâÔºâÔºå‰ª•ÂèäËøô‰∫õËØçÁöÑÂÖàÂêéÈ°∫Â∫èÔºàÁõ∏ÂØπ‰ΩçÁΩÆÔºâÂÖ±Âêå‰ΩúÁî®‰∏ãÔºå‰∏ã‰∏Ä‰∏™ËØçÊúÄÂèØËÉΩÊòØ‰ªÄ‰πà
2,Ê®°ÂûãÂà§Êñ≠ ‚Äú‰∏ã‰∏Ä‰∏™ËØçÊòØ‰ªÄ‰πà‚Äù Êó∂ÔºåÂøÖÈ°ª‰æùËµñ„ÄåËØçÁöÑÈ°∫Â∫è„Äç‚Äî‚Äî ËÄå ‚ÄúÁõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ª‚Äù Â∞±ÊòØÊ®°ÂûãËØÜÂà´ ‚ÄúÈ°∫Â∫è‚Äù ÁöÑÂîØ‰∏Ä‰æùÊçÆÔºàÈÄöËøá‰ΩçÁΩÆÁºñÁ†ÅÂÆûÁé∞Ôºâ
'''

class MiniMindConfig(PretrainedConfig):
    '''
    transformers ÁöÑ AutoConfig ‰ºöÈÄöËøá model_type Ëá™Âä®ÂåπÈÖçÂØπÂ∫îÁöÑÈÖçÁΩÆÁ±ªÔºå
    ÊòØÁîüÊÄÅÂÖºÂÆπÁöÑÊ†∏ÂøÉÊ†áËØÜÔºàÈúÄ‰∏éÊ®°ÂûãÊ≥®ÂÜåÊó∂ÁöÑÂêçÁß∞‰∏ÄËá¥Ôºâ
    '''
    model_type = "minimind"

    def __init__(
            self,
            # Dropout Ê¶ÇÁéáÔºàÈò≤Ê≠¢ËøáÊãüÂêàÔºâÔºöÊ®°Âûã‰∏≠ÈöèÊú∫‰∏¢ÂºÉÈÉ®ÂàÜÁ•ûÁªèÂÖÉÁöÑÊØî‰æãÔºå0.0 Ë°®Á§∫‰∏ç‰ΩøÁî® Dropout„ÄÇ
            dropout: float = 0.0,
            # Âè•È¶ñÊ†áËÆ∞ÔºàBegin Of SequenceÔºâÁöÑ token IDÔºöÊñáÊú¨ÁºñÁ†ÅÊó∂Áî®‰∫éÊ†áËØÜÂè•Â≠êÂºÄÂ§¥ÔºåÈúÄ‰∏éËØçË°®ÔºàvocabÔºâ‰∏≠ÁöÑ ID ‰∏ÄËá¥„ÄÇ
            bos_token_id: int = 1,
            # Âè•Â∞æÊ†áËÆ∞ÔºàEnd Of SequenceÔºâÁöÑ token IDÔºöÊñáÊú¨ÁºñÁ†ÅÊó∂Áî®‰∫éÊ†áËØÜÂè•Â≠êÁªìÊùüÔºåÈúÄ‰∏éËØçË°®‰∏ÄËá¥„ÄÇ
            eos_token_id: int = 2,
            # ÈöêËóèÂ±ÇÊøÄÊ¥ªÂáΩÊï∞Ôºösilu Âç≥ Sigmoid Linear UnitÔºàœÉ(x)„ÉªxÔºâÔºåÊòØÂ§ßÊ®°ÂûãÂ∏∏Áî®ÊøÄÊ¥ªÂáΩÊï∞ÔºàÊØî ReLU Êõ¥Âπ≥ÊªëÔºâÔºåÊîØÊåÅ relu/gelu Á≠âÂÖ∂‰ªñÈÄâÈ°π„ÄÇ
            hidden_act: str = 'silu',
            # ÈöêËóèÂ±ÇÁª¥Â∫¶ÔºöTransformer ÁºñÁ†ÅÂô® / Ëß£Á†ÅÂô®‰∏≠ÊØè‰∏™ token ÁöÑÂêëÈáèÁª¥Â∫¶ÔºàÊ†∏ÂøÉË∂ÖÂèÇÊï∞ÔºâÔºåÂÜ≥ÂÆöÊ®°ÂûãÂÆπÈáèÔºåÈÄöÂ∏∏‰∏∫ 2 ÁöÑÂπÇÔºàÂ¶Ç 512„ÄÅ1024Ôºâ„ÄÇ
            hidden_size: int = 512,
            # Feed-Forward ÁΩëÁªú‰∏≠Èó¥Â±ÇÁª¥Â∫¶ÔºöTransformer ‰∏≠„ÄåËá™Ê≥®ÊÑèÂäõÂ±ÇÂêé„ÄçÁöÑÂÖ®ËøûÊé•Â±ÇÁª¥Â∫¶ÔºåÈªòËÆ§ None Êó∂ÈÄöÂ∏∏Êåâ hidden_size * 4 ËÆ°ÁÆóÔºàÂ§ßÊ®°ÂûãÂ∏∏ËßÅËÆæËÆ°Ôºâ„ÄÇ
            intermediate_size: int = None,
            # ÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶ÔºöÊ®°ÂûãÊîØÊåÅÁöÑÊúÄÈïøËæìÂÖ•ÊñáÊú¨ÈïøÂ∫¶Ôºàtoken Êï∞ÔºâÔºå32768 Ë°®Á§∫ÊîØÊåÅ 32k ÈïøÊñáÊú¨ÔºåÈúÄ‰∏é‰ΩçÁΩÆÁºñÁ†ÅÔºàPositional EmbeddingÔºâÁöÑÁª¥Â∫¶ÂåπÈÖç„ÄÇ
            max_position_embeddings: int = 32768,
            # Ëá™Ê≥®ÊÑèÂäõÂ§¥Êï∞ÔºöÂ∞Ü hidden_size ÊãÜÂàÜ‰∏∫Â§ö‰∏™Â§¥Âπ∂Ë°åËÆ°ÁÆóÊ≥®ÊÑèÂäõÔºåÊèêÂçáÊ®°ÂûãÂØπ‰∏çÂêåÁâπÂæÅÁöÑÊçïÊçâËÉΩÂäõÔºàÈúÄÊª°Ë∂≥ hidden_size % num_attention_heads == 0ÔºåÂê¶ÂàôÊó†Ê≥ïÂùáÂàÜÁª¥Â∫¶Ôºâ„ÄÇ
            num_attention_heads: int = 8,
            # 	Transformer ÈöêËóèÂ±ÇÊï∞ÈáèÔºàÂç≥ÁºñÁ†ÅÂô® / Ëß£Á†ÅÂô®ÁöÑÂ±ÇÊï∞ÔºâÔºåÂ±ÇÊï∞Ë∂äÂ§öÊ®°ÂûãÊãüÂêàËÉΩÂäõË∂äÂº∫Ôºå‰ΩÜËÆ≠ÁªÉÊàêÊú¨Ë∂äÈ´ò„ÄÇ
            num_hidden_layers: int = 8,
            # KV Â§¥Êï∞ÔºàÁî®‰∫éÂàÜÁªÑÊ≥®ÊÑèÂäõ / FlashAttentionÔºâÔºöÂú®È´òÊïàÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÔºåÂ∞Ü Key/Value ÊäïÂΩ±Âà∞ fewer ‰∏™Â§¥‰∏äÔºàÂ¶Ç 2 ‰∏™ÔºâÔºåÂáèÂ∞ëËÆ°ÁÆóÈáèÔºàÈúÄÊª°Ë∂≥ num_attention_heads % num_key_value_heads == 0Ôºâ„ÄÇ
            num_key_value_heads: int = 2,
            # ËØçË°®Â§ßÂ∞èÔºöÊ®°ÂûãÊîØÊåÅÁöÑÂîØ‰∏Ä token Êï∞ÈáèÔºàÂåÖÊã¨Â≠óÁ¨¶„ÄÅÂ≠êËØçÁ≠âÔºâÔºåÈúÄ‰∏éËØçË°®Êñá‰ª∂ÔºàÂ¶Ç vocab.jsonÔºâÁöÑÂ§ßÂ∞è‰∏ÄËá¥„ÄÇ
            vocab_size: int = 6400,
            # RMSNorm ÂΩí‰∏ÄÂåñÁöÑÊûÅÂ∞èÂÄºÔºöÁî®‰∫éÈÅøÂÖçÂàÜÊØç‰∏∫ 0ÔºåRMSNorm ÊòØÂ§ßÊ®°ÂûãÂ∏∏Áî®ÁöÑÂΩí‰∏ÄÂåñÊñπÂºèÔºàÊØî LayerNorm ËÆ°ÁÆóÊõ¥È´òÊïàÔºâ„ÄÇ
            rms_norm_eps: float = 1e-05,
            # RoPE ‰ΩçÁΩÆÁºñÁ†ÅÁöÑ theta ÂèÇÊï∞ÔºöRoPEÔºàRotary Position EmbeddingÔºâÈÄöËøáÊóãËΩ¨Áü©ÈòµÊ≥®ÂÖ•‰ΩçÁΩÆ‰ø°ÊÅØÔºåtheta ÂÜ≥ÂÆö‰ΩçÁΩÆÁºñÁ†ÅÁöÑÂë®ÊúüÔºàÂÄºË∂äÂ§ßÔºåÂë®ÊúüË∂äÈïøÔºåÈÄÇÂêàÈïøÊñáÊú¨Ôºâ„ÄÇ
            rope_theta: int = 1000000.0,
            # ÊòØÂê¶ÂêØÁî® RoPE ÈïøÂ∫¶Â§ñÊé®ÔºöÊé®ÁêÜÊó∂Ëã•ËæìÂÖ•ÊñáÊú¨ÈïøÂ∫¶Ë∂ÖËøá max_position_embeddingsÔºåÈÄöËøáÁº©Êîæ RoPE ÂèÇÊï∞ÈÅøÂÖç‰ΩçÁΩÆÁºñÁ†ÅÂ§±ÊïàÔºàÂ¶Ç YARN ÊñπÊ≥ïÔºâ„ÄÇ
            inference_rope_scaling: bool = False,
            # ÊòØÂê¶ÂêØÁî® FlashAttentionÔºöFacebook ÊèêÂá∫ÁöÑÈ´òÊïàÊ≥®ÊÑèÂäõÂÆûÁé∞ÔºåÂ§ßÂπÖÈôç‰ΩéÊòæÂ≠òÂç†Áî®ÂíåËÆ°ÁÆóÊó∂Èó¥ÔºåÊòØÂ§ßÊ®°ÂûãËÆ≠ÁªÉ / Êé®ÁêÜÁöÑÂ∏∏Áî®‰ºòÂåñ„ÄÇ
            flash_attn: bool = True,
            ####################################################
            # Here are the specific configurations of MOE
            # When use_moe is false, the following is invalid
            ####################################################
            # ÊòØÂê¶ÂêØÁî® MOE ÁªìÊûÑÔºöTrue Ë°®Á§∫Ê®°Âûã‰ΩøÁî®Ê∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑÔºåFalse Âàô‰∏∫ÊôÆÈÄö Transformer„ÄÇ
            
            # ‰∏çÂ∞±ÊòØÂ§ö‰∏™ÂÖ®ËøûÊé•Â±ÇÔºà‰∏çÂêåÂÖ®ËøûÊé•Â±Ç‰ª£Ë°®ÊîæÂ§ß‰∏çÂêåÊñπÈù¢ÁöÑÁâπÂæÅÔºâÔºå
            # Áî®‰∏Ä‰∏™ÁΩëÁªúÔºàË∑ØÁî±Âô®ÔºâÊ†πÊçÆsigmiodÔºåÂÜ≥ÂÆöÂì™Âá†‰∏™ÂÖ®ËøûÊé•Â±ÇÊé•Êî∂ËæìÂÖ•ÔºåÂπ∂ÂíåÈÄöÁî®ÁöÑÂÖ®ËøûÊé•Â±ÇÂä†ÊùÉÊ±ÇÂíå
            use_moe: bool = False,
            # ÊØè‰∏™ token ÈÄâÊã©ÁöÑ‰∏ìÂÆ∂Êï∞ÔºöMOE ‰∏≠ÊØè‰∏™ token ‰ªÖÁî± top-k ‰∏™‰∏ìÂÆ∂Â§ÑÁêÜÔºàÂ¶Ç 2 ‰∏™ÔºâÔºåÂπ≥Ë°°ÊÄßËÉΩÂíåËÆ°ÁÆóÈáè„ÄÇ
            num_experts_per_tok: int = 2,
            # ÂèØË∑ØÁî±‰∏ìÂÆ∂ÊÄªÊï∞ÔºöÊ®°Âûã‰∏≠Áã¨Á´ãÁöÑ‰∏ìÂÆ∂ÁΩëÁªúÊï∞ÈáèÔºàÂ¶Ç 4 ‰∏™ÔºâÔºåÊØè‰∏™‰∏ìÂÆ∂Ë¥üË¥£Â§ÑÁêÜÁâπÂÆöÁ±ªÂûãÁöÑ token„ÄÇ
            n_routed_experts: int = 4,
            # ÂÖ±‰∫´‰∏ìÂÆ∂Êï∞ÈáèÔºöÊâÄÊúâ token ÈÉΩ‰ºöÁªèËøáÁöÑ„ÄåÂÖ±‰∫´‰∏ìÂÆ∂„ÄçÔºàÂå∫Âà´‰∫é„ÄåÂèØË∑ØÁî±‰∏ìÂÆ∂„ÄçÔºâÔºåÊèêÂçáÊ®°ÂûãÊ≥õÂåñËÉΩÂäõÔºàÈÅøÂÖçÈÉ®ÂàÜ‰∏ìÂÆ∂Ë¢´Èó≤ÁΩÆÔºâ„ÄÇ
            n_shared_experts: int = 1,
            # ‰∏ìÂÆ∂ÈÄâÊã©ÁöÑËØÑÂàÜÂáΩÊï∞ÔºöËÆ°ÁÆóÊØè‰∏™ token ‰∏é‰∏ìÂÆ∂ÁöÑÂåπÈÖçÂ∫¶Ôºåsoftmax ‰ºöÂ∞ÜËØÑÂàÜÂΩí‰∏ÄÂåñ‰∏∫Ê¶ÇÁéáÔºåÂÖ∂‰ªñÂèØÈÄâÂ¶Ç sigmoid„ÄÇ
            scoring_func: str = 'softmax',
            # ËæÖÂä©ÊçüÂ§±ÁöÑÊùÉÈáçÔºöMOE ‰∏≠‰∏∫ÈÅøÂÖç„Äå‰∏ìÂÆ∂Èó≤ÁΩÆ„ÄçÔºàÈÉ®ÂàÜ‰∏ìÂÆ∂Âá†‰πé‰∏çË¢´ÈÄâÊã©ÔºâÔºåÊ∑ªÂä†ËæÖÂä©ÊçüÂ§±ÔºàÂ¶Ç‰∏ìÂÆ∂ÂùáË°°ÊçüÂ§±ÔºâÔºåalpha ÊéßÂà∂ËæÖÂä©ÊçüÂ§±Âú®ÊÄªÊçüÂ§±‰∏≠ÁöÑÂç†ÊØî„ÄÇ
            aux_loss_alpha: float = 0.1,
            # ÊòØÂê¶Âú®Â∫èÂàóÁ∫ßÂà´ËÆ°ÁÆóËæÖÂä©ÊçüÂ§±ÔºöTrue Ë°®Á§∫Âü∫‰∫éÊï¥‰∏™Â∫èÂàóÁöÑ‰∏ìÂÆ∂ÈÄâÊã©ÊÉÖÂÜµËÆ°ÁÆóÂùáË°°ÊçüÂ§±ÔºåFalse ÂàôÊåâÂçï‰∏™ token ËÆ°ÁÆó„ÄÇ
            seq_aux: bool = True,
            # ÊòØÂê¶Ê†áÂáÜÂåñ top-k ‰∏ìÂÆ∂ÁöÑÊ¶ÇÁéáÔºöTrue ‰ºöÂ∞ÜÈÄâ‰∏≠ÁöÑ k ‰∏™‰∏ìÂÆ∂ÁöÑÊ¶ÇÁéáÈáçÊñ∞ÂΩí‰∏ÄÂåñÔºåÁ°Æ‰øùÊ¶ÇÁéáÂíå‰∏∫ 1ÔºåÊèêÂçáÁ®≥ÂÆöÊÄß„ÄÇ
            norm_topk_prob: bool = True,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        # Â§ñÊé®ÈïøÂ∫¶ = factor * original_max_position_embeddings
        # ‰øÆÊîπÊ®°ÂûãÂØπ ‚Äú‰ΩçÁΩÆ‰ø°ÊÅØ‚Äù ÁöÑÂ§ÑÁêÜÊñπÂºèÔºåËÆ©Ê®°ÂûãËØØ‰ª•‰∏∫ ‚ÄúË∂ÖÈïøÊñáÊú¨ÁöÑ‰ΩçÁΩÆ‚Äù ‰ªçÂú®Ëá™Â∑±ÁÜüÊÇâÁöÑ ‚ÄúËÆ≠ÁªÉÁ™óÂè£‚Äù ÂÜÖÔºå‰ªéËÄåÊ≠£Â∏∏ÁêÜËß£ÂÜÖÂÆπ

        self.rope_scaling = {
            "beta_fast": 4,
            "beta_slow": 1,
            "factor": 4,
            "original_max_position_embeddings": 2048,
            "type": "yarn"
        } if self.inference_rope_scaling else None
        self.flash_attn = flash_attn
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # ÊØè‰∏™tokenÈÄâÊã©ÁöÑ‰∏ìÂÆ∂Êï∞Èáè
        self.n_routed_experts = n_routed_experts  # ÊÄªÁöÑ‰∏ìÂÆ∂Êï∞Èáè
        self.n_shared_experts = n_shared_experts  # ÂÖ±‰∫´‰∏ìÂÆ∂
        self.scoring_func = scoring_func  # ËØÑÂàÜÂáΩÊï∞ÔºåÈªòËÆ§‰∏∫'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # ËæÖÂä©ÊçüÂ§±ÁöÑalphaÂèÇÊï∞
        self.seq_aux = seq_aux  # ÊòØÂê¶Âú®Â∫èÂàóÁ∫ßÂà´‰∏äËÆ°ÁÆóËæÖÂä©ÊçüÂ§±
        self.norm_topk_prob = norm_topk_prob  # ÊòØÂê¶Ê†áÂáÜÂåñtop-kÊ¶ÇÁéá


# üìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìò
#                                             MiniMind Model
# üìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìòüìò

# Python ÂÜÖÁΩÆÁöÑÊï∞Â≠¶Â∑•ÂÖ∑Â∫ìÔºåÊèê‰æõÂü∫Á°ÄÊï∞Â≠¶ËøêÁÆóÊîØÊåÅ
import math
# PyTorch Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÂ∫ìÔºåÊòØÊâÄÊúâÊ®°ÂûãËÆ≠ÁªÉ / Êé®ÁêÜÁöÑÂü∫Á°Ä,Êèê‰æõÂ§öÁßçÂèÇÊï∞ÂàùÂßãÂåñÊñπÊ≥ïÔºåÈÅøÂÖçÊ®°ÂûãËÆ≠ÁªÉÊó∂Âõ†ÂèÇÊï∞ÂàùÂßãÂÄº‰∏çÂΩìÂØºËá¥ÁöÑÊ¢ØÂ∫¶Ê∂àÂ§± / ÁàÜÁÇ∏
# Êèê‰æõÂº†ÈáèÔºàTensorÔºâÊìç‰Ωú„ÄÅËá™Âä®Ê±ÇÂØºÔºàAutogradÔºâ„ÄÅGPU Âä†ÈÄü„ÄÅÁ•ûÁªèÁΩëÁªúÊ®°ÂùóÔºànnÔºâÁ≠âÊ†∏ÂøÉËÉΩÂäõÔºåÂêéÁª≠ÊâÄÊúâÊ®°ÂûãÁöÑÂèÇÊï∞ÔºàÊùÉÈáçÔºâ„ÄÅËæìÂÖ•Êï∞ÊçÆÈÉΩ‰ª• PyTorch Âº†ÈáèÂΩ¢ÂºèÂ≠òÂÇ®ÂíåÂ§ÑÁêÜ„ÄÇ
import torch
# PyTorch ‰∏≠Á•ûÁªèÁΩëÁªúÂèÇÊï∞ÁöÑÂàùÂßãÂåñÂ∑•ÂÖ∑Ê®°ÂùóÔºåÁº©ÂÜô‰∏∫ initÔºàÁ∫¶ÂÆö‰øóÊàêÁöÑÁÆÄÂÜôÔºåÊñπ‰æøË∞ÉÁî®Ôºâ
import torch.nn.init as init
# Êèê‰æõÊó†Áä∂ÊÄÅÁöÑÁ•ûÁªèÁΩëÁªúÊìç‰ΩúÔºàÂç≥Êìç‰ΩúÊú¨Ë∫´‰∏çÂ≠òÂÇ®ÂèÇÊï∞Ôºå‰ªÖÊé•Êî∂ËæìÂÖ•ÂíåÂèÇÊï∞ËÆ°ÁÆóËæìÂá∫Ôºâ
# Ë∞ÉÁî®ÊøÄÊ¥ªÂáΩÊï∞ÔºàF.silu„ÄÅF.geluÔºâ„ÄÅÊçüÂ§±ÂáΩÊï∞ÔºàF.cross_entropy„ÄÅF.mse_lossÔºâ
import torch.nn.functional as F
# Êèê‰æõÂ∞ÅË£ÖÂ•ΩÁöÑÂèØËÆ≠ÁªÉÊ®°ÂùóÔºàÁ±ªÔºâÔºåËøô‰∫õÊ®°Âùó‰ºöËá™Âä®ÁÆ°ÁêÜÂÜÖÈÉ®ÂèÇÊï∞ÔºàÊùÉÈáç„ÄÅÂÅèÁΩÆÔºâÔºåÊîØÊåÅËá™Âä®Ê±ÇÂØºÂíåÂèÇÊï∞‰ºòÂåñ„ÄÇ
# ÂÆö‰πâÊ®°ÂûãÂ±ÇÁªìÊûÑÔºå‰æãÂ¶ÇÂÖ®ËøûÊé•Â±ÇÔºànn.LinearÔºâ„ÄÅÂΩí‰∏ÄÂåñÂ±ÇÔºànn.RMSNormÔºâ„ÄÅDropout Â±ÇÔºànn.DropoutÔºâ„ÄÅEmbedding Â±ÇÔºànn.EmbeddingÔºâÁ≠âÔºåÂêéÁª≠ÊûÑÂª∫ Transformer/MOE Ê®°ÂûãÁöÑÂ±ÇÈÉΩ‰ºö‰æùËµñ nn Ê®°Âùó„ÄÇ
from torch import nn
# ÂΩìÈÖçÁΩÆ‰∏≠ÊåáÂÆö hidden_act='silu' Êó∂ÔºåÂèØÈÄöËøá ACT2FN[config.hidden_act] Áõ¥Êé•Ëé∑ÂèñÂØπÂ∫îÁöÑÊøÄÊ¥ªÂáΩÊï∞ÔºàÊó†ÈúÄÊâãÂä®Âà§Êñ≠Â≠óÁ¨¶‰∏≤ÂØπÂ∫îÁöÑÂáΩÊï∞Ôºâ
from transformers.activations import ACT2FN
from typing import Optional, Tuple, List, Union
# ‰ªé transformers Â∫ìÂØºÂÖ•ÊûÑÂª∫Ëá™ÂÆö‰πâÊ®°ÂûãÁöÑÊ†∏ÂøÉÂü∫Á±ªÔºåÊòØÂÖºÂÆπ transformers ÁîüÊÄÅÁöÑÂÖ≥ÈîÆ„ÄÇ
# PreTrainedModel: ÊâÄÊúâÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÁà∂Á±ªÔºåÊèê‰æõÊ®°ÂûãÂä†ËΩΩ / ‰øùÂ≠òÔºàfrom_pretrained/save_pretrainedÔºâ„ÄÅËÆæÂ§áËøÅÁßªÔºàto(device)Ôºâ„ÄÅÂèÇÊï∞ÂÜªÁªìÁ≠âÈÄöÁî®ÂäüËÉΩÔºåËá™ÂÆö‰πâÊ®°ÂûãÔºàÂ¶Ç MiniMindModelÔºâÈúÄÁªßÊâøÊ≠§Á±ª„ÄÇ
# GenerationMixinÔºö ÁîüÊàêÂºèÊ®°ÂûãÁöÑÊ∑∑ÂÖ•Á±ªÔºàMixinÔºâÔºåÊèê‰æõÊñáÊú¨ÁîüÊàêÁöÑÊ†∏ÂøÉÊñπÊ≥ïÔºàÂ¶Ç generate()ÔºâÔºåÂåÖÂê´Ë¥™ÂøÉÊêúÁ¥¢„ÄÅÊùüÊêúÁ¥¢ÔºàBeam SearchÔºâÁ≠âÁîüÊàêÁ≠ñÁï•ÔºåËÆ©Ëá™ÂÆö‰πâÊ®°ÂûãÊó†ÈúÄÊâãÂä®ÂÆûÁé∞ÁîüÊàêÈÄªËæë„ÄÇ
# ‰πãÂâçËÆ≤Ëß£ËøáÁöÑÈÖçÁΩÆÁ±ªÁà∂Á±ªÔºåÊ≠§Â§ÑÂØºÂÖ•ÊòØ‰∏∫‰∫ÜÂú®Ê®°ÂûãÁ±ª‰∏≠Êé•Êî∂ÈÖçÁΩÆÂÆû‰æãÔºàÂ¶Ç __init__(self, config: PretrainedConfig)ÔºâÔºåÁ°Æ‰øùÊ®°Âûã‰∏éÈÖçÁΩÆÁöÑËÅîÂä®„ÄÇ
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
'''
Áªü‰∏ÄÁîüÊàêÂºèÊ®°ÂûãÁöÑËæìÂá∫Ê†ºÂºèÔºåÂ∞ÜÊ®°ÂûãÁöÑÊ†∏ÂøÉËæìÂá∫ÔºàÈ¢ÑÊµã logits„ÄÅÈöêËóèÁä∂ÊÄÅ„ÄÅÊ≥®ÊÑèÂäõÊùÉÈáçÁ≠âÔºâÂ∞ÅË£ÖÊàê‰∏Ä‰∏™ÂÖ∑ÂêçÂÖÉÁªÑÔºàNamed TupleÔºâÔºåÊñπ‰æøÂêéÁª≠Ë∞ÉÁî®ÔºàÂ¶ÇËÆ°ÁÆóÊçüÂ§±„ÄÅÁîüÊàêÊñáÊú¨Êó∂Ëé∑Âèñ‰∏≠Èó¥ÁªìÊûúÔºâ„ÄÇ
ËæìÂá∫Á±ªÂåÖÂê´ÁöÑÂÖ≥ÈîÆÂ±ûÊÄßÔºàÊåâÈúÄ‰ΩøÁî®ÔºâÔºö
logits: Ê®°ÂûãÊúÄÁªàÈ¢ÑÊµãÁöÑ token Ê¶ÇÁéáÂàÜÂ∏ÉÔºàshape: [batch_size, seq_len, vocab_size]ÔºâÔºåÁî®‰∫éËÆ°ÁÆóÊçüÂ§±ÊàñÈááÊ†∑‰∏ã‰∏Ä‰∏™ tokenÔºõ
past_key_values: ÁºìÂ≠òÁöÑÊ≥®ÊÑèÂäõÂ±Ç Key/Value Âº†ÈáèÔºåÁî®‰∫éÂ¢ûÈáèÁîüÊàêÔºàÈÅøÂÖçÈáçÂ§çËÆ°ÁÆóÂ∑≤ÁîüÊàê token ÁöÑÊ≥®ÊÑèÂäõÔºåÊèêÂçáÁîüÊàêÈÄüÂ∫¶ÔºâÔºõ
hidden_states: Ê®°ÂûãÂêÑÂ±ÇÁöÑÈöêËóèÁä∂ÊÄÅÔºàÂèØÈÄâÔºâÔºåÁî®‰∫éÁâπÂæÅÊèêÂèñÊàñË∞ÉËØïÔºõ
attentions: ÂêÑÊ≥®ÊÑèÂäõÂ±ÇÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÔºàÂèØÈÄâÔºâÔºåÁî®‰∫éÂèØËßÜÂåñÊàñÂàÜÊûêÊ®°ÂûãÊ≥®ÊÑèÂäõÂàÜÂ∏É„ÄÇ
'''
from transformers.modeling_outputs import CausalLMOutputWithPast

'''
‰∏∫‰ªÄ‰πàÁªßÊâø nn.ModuleÔºö
Ëé∑Âæó PyTorch ÂÜÖÁΩÆÁöÑÂèÇÊï∞ÁÆ°ÁêÜÔºàÂ¶Ç nn.Parameter Ëá™Âä®Ê≥®ÂÜå‰∏∫ÂèØËÆ≠ÁªÉÂèÇÊï∞Ôºâ„ÄÅËÆæÂ§áËøÅÁßªÔºàto(device)Ôºâ„ÄÅÂâçÂêë‰º†Êí≠Êé•Âè£Ôºàforward ÊñπÊ≥ïÔºâÁ≠âÊ†∏ÂøÉÂäüËÉΩÔºõ
Á°Æ‰øùËØ•Â±ÇËÉΩÂÉè nn.Linear„ÄÅnn.Dropout ‰∏ÄÊ†∑ÔºåÂµåÂÖ•Âà∞ÂÆåÊï¥ÁöÑÁ•ûÁªèÁΩëÁªú‰∏≠‰ΩøÁî®„ÄÇ

ÈÄöËøá ‚ÄúÊ†áÂáÜÂåñ + Ëá™ÈÄÇÂ∫îÁº©Êîæ‚ÄùÔºåËÆ©Ê®°Âûã‰∏≠ÊØè‰∏™ token ÁöÑÈöêËóèÂêëÈáèÂπÖÂ∫¶‰øùÊåÅ‰∏ÄËá¥ÔºåÈÅøÂÖçÂõ†Êï∞ÂÄºËøáÂ§ß / ËøáÂ∞èÂØºËá¥ÁöÑËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÔºàÂ¶ÇÊ¢ØÂ∫¶Ê∂àÂ§± / ÁàÜÁÇ∏Ôºâ„ÄÇ
Áõ∏ÊØî‰º†Áªü LayerNormÔºåRMSNorm Â∞ë‰∫Ü„ÄåÂáèÂùáÂÄº„ÄçÁöÑÊ≠•È™§ÔºåËÆ°ÁÆóÈáèÊõ¥Â∞è„ÄÅÊòæÂ≠òÂç†Áî®Êõ¥‰ΩéÔºåÊòØÂ§ßÊ®°ÂûãÔºàÂ¶Ç LLaMA„ÄÅGPT-4ÔºâÁöÑ‰∏ªÊµÅÈÄâÊã©„ÄÇ
Âú® Transformer Â±ÇÔºàÊàñ MOE ‰∏ìÂÆ∂Â±ÇÔºâÁöÑËæìÂÖ• / ËæìÂá∫Â§ÑÊèíÂÖ•ËØ•Â±ÇÔºå‰º†ÂÖ• eps=config.rms_norm_eps Âç≥ÂèØ
'''
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    # ÂÆö‰πâÊ†∏ÂøÉÁöÑÂΩí‰∏ÄÂåñËÆ°ÁÆóÈÄªËæëÔºåÁî®‰∏ãÂàíÁ∫ø _ ÂºÄÂ§¥Ë°®Á§∫„ÄåÂÜÖÈÉ®ËæÖÂä©ÂáΩÊï∞„ÄçÔºà‰∏çÂª∫ËÆÆÂ§ñÈÉ®Áõ¥Êé•Ë∞ÉÁî®Ôºå‰ªÖÂú® forward ‰∏≠‰ΩøÁî®Ôºâ„ÄÇ
    '''
    torch.rsqrt: ËÆ°ÁÆóÂπ≥ÊñπÊ†πÁöÑÂÄíÊï∞
    x.pow(2): ËÆ°ÁÆóËæìÂÖ•Âº†ÈáèÊØè‰∏™‰ΩçÁΩÆÁöÑÂπ≥ÊñπÔºàÂ¶Ç x=3 ÂèòÊàê 9Ôºåx=-2 ÂèòÊàê 4ÔºâÔºåÁõÆÁöÑÊòØÊ∂àÈô§Ê≠£Ë¥üÂè∑ÂΩ±ÂìçÔºåËÅöÁÑ¶Êï∞ÂÄºÂ§ßÂ∞è
    x.pow(2).mean(-1, keepdim=True) :-1 Ë°®Á§∫„ÄåÊúÄÂêé‰∏Ä‰∏™Áª¥Â∫¶„ÄçÔºàÂç≥ dim Áª¥Â∫¶ÔºâÔºåkeepdim=True Ë°®Á§∫‰øùÊåÅÁª¥Â∫¶‰∏çÂèòÔºàËæìÂÖ• [32,128,512] ‚Üí ËæìÂá∫ [32,128,1]ÔºâÔºåÈÅøÂÖçÂπøÊí≠ËÆ°ÁÆóÂá∫ÈîôÔºõÁªìÊûúÊòØÊØè‰∏™ token 512 Áª¥ÂêëÈáèÁöÑ„ÄåÂπ≥ÊñπÂùáÂÄº„ÄçÔºàË°°ÈáèËØ• token ÂêëÈáèÁöÑÊï¥‰ΩìÂπÖÂ∫¶Ôºâ
    x * ... Áî®ËæìÂÖ•Âº†Èáè x ‰πò‰ª•ÂΩí‰∏ÄÂåñÁ≥ªÊï∞ÔºåÊúÄÁªàÂæóÂà∞„ÄåÂùáÂÄº‰∏∫ 0„ÄÅÊñπÂ∑ÆËøë‰ºº‰∏∫ 1„ÄçÁöÑÊ†áÂáÜÂåñÂêëÈáèÔºàÊ∂àÈô§‰∏çÂêå token ÂêëÈáèÂπÖÂ∫¶Â∑ÆÂºÇÁöÑÂΩ±ÂìçÔºâ
    '''
    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    # ÂÆö‰πâÊï∞ÊçÆÁöÑÂâçÂêëÊµÅÂä®ÈÄªËæë
    # .type_as(x) ÈÅøÂÖçÁ±ªÂûã‰∏çÂåπÈÖçÔºàÂ¶ÇËæìÂÖ•ÊòØ float16ÔºåÊ†áÂáÜÂåñÂêé‰ªçËΩ¨Âõû float16Ôºå‰øùËØÅÂêéÁª≠ËÆ°ÁÆóÂÖºÂÆπÔºâ
    # self.weight * ... Áî®ÂèØËÆ≠ÁªÉÁöÑ weightÔºàshape [dim]ÔºâÂØπÊ†áÂáÜÂåñÂêëÈáèÁöÑÊØè‰∏™Áª¥Â∫¶ÈÄêÂÖÉÁ¥†Áõ∏‰πòÔºàÂπøÊí≠Êú∫Âà∂ÔºâÔºåÂÆûÁé∞ ‚ÄúËá™ÈÄÇÂ∫îÁº©Êîæ‚Äù
    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)

'''
Yarn ÈïøÂ∫¶Â§ñÊé®

Á¨¨‰∏ÄÊ≠•ÔºöÂë®ÊúüÊò†Â∞ÑÂÖ¨Âºè
p' = p mod L + s * (L/K)   s=floor(p/L)

p mod L :‰øùËØÅÂßãÁªàÂú®‰∏Ä‰∏™Âë®ÊúüÂÜÖ
s * (L/K): Âå∫ÂàÜ‰∏çÂêåÂë®Êúü‰∏≠Áõ∏Âêå‰ΩçÁΩÆ
----------------------
ÊãÜËß£ 1Ôºöp mod LÔºàÂèñ‰ΩôËøêÁÆóÔºâ‚Üí ÂØπÂ∫î ‚ÄúÂë®ÊúüÂÜÖÁöÑÂü∫Á°Ä‰ΩçÁΩÆ‚Äù„ÄÇÊØîÂ¶Ç L=2000Ôºåp=10000 Êó∂Ôºå10000 mod 2000 = 0ÔºåÂç≥Á¨¨ 10000 ‰∏™ token ÂØπÂ∫î ‚ÄúÁ¨¨ 5 ‰∏™Âë®ÊúüÁöÑÁ¨¨ 0 ‰∏™‰ΩçÁΩÆ‚ÄùÔºàÁ±ª‰ºº ‚ÄúÁ¨¨ 5 ËΩÆÁöÑÁ¨¨ 1 Êú¨‰π¶‚ÄùÔºâÔºõ
‚úÖ ÁîüÊ¥ªÂØπÂ∫îÔºöÁÆ°ÁêÜÂëò‰∏çÁî®ËÆ∞ ‚ÄúÁ¨¨ 10000 Êú¨‚ÄùÔºåÂè™ËÆ∞ ‚ÄúËøôÊòØÂΩìÂâçÂë®ÊúüÁöÑÁ¨¨ 0 Êú¨‚ÄùÔºåÂíåÁ¨¨ 1 ‰∏™Âë®ÊúüÁöÑ ‚ÄúÁ¨¨ 0 Êú¨‚ÄùÔºàÂç≥Á¨¨ 1 Êú¨‰π¶ÔºâÁºñÁ†ÅÈÄªËæë‰∏ÄËá¥Ôºå‰∏ç‰ºöÊáµ„ÄÇ
ÊãÜËß£ 2Ôºös = floor(p / L)ÔºàÂêë‰∏ãÂèñÊï¥Ôºâ‚Üí ÂØπÂ∫î ‚ÄúÂë®ÊúüÂ∫èÂè∑‚Äù„ÄÇp=10000 Êó∂Ôºås=10000/2000=5ÔºåÂç≥Á¨¨ 5 ‰∏™Âë®ÊúüÔºõ
ÊãÜËß£ 3ÔºöK ÊòØ ‚ÄúÂë®ÊúüÂàÜÁªÑÊï∞‚ÄùÔºàYARN È¢ÑËÆæÁöÑË∂ÖÂèÇÊï∞ÔºåÊØîÂ¶Ç 4Ôºâ‚Üí Áªô‰∏çÂêåÂë®ÊúüÂä† ‚ÄúËΩªÂæÆÂå∫ÂàÜ‚ÄùÔºåÈÅøÂÖçÊ®°ÂûãÊ∑∑Ê∑Ü ‚ÄúÁ¨¨ 1 Âë®ÊúüÁöÑÁ¨¨ 0 Êú¨‚Äù Âíå ‚ÄúÁ¨¨ 5 Âë®ÊúüÁöÑÁ¨¨ 0 Êú¨‚ÄùÔºå‰ΩÜÂå∫ÂàÜÂ∫¶ÂæàÂ∞èÔºå‰∏çÂΩ±ÂìçÊ®°ÂûãÂØπ ‚ÄúÂë®ÊúüÂÜÖÈ°∫Â∫è‚Äù ÁöÑËØÜÂà´„ÄÇ
Ôºà2ÔºâÁõ∏ÂØπ‰ΩçÁΩÆË°•ÂÖÖÔºàÂÖ¨ÂºèÈöêÂê´ÈÄªËæëÔºâ
‰º†ÁªüÁºñÁ†ÅÂè™Áúã pÔºàÁªùÂØπ‰ΩçÁΩÆÔºâÔºåYARN ÈÄöËøáÂë®ÊúüÊò†Â∞ÑÂêéÔºå‰∏§‰∏™ token ÁöÑ„ÄåÁõ∏ÂØπ‰ΩçÁΩÆ„ÄçÂèØ‰ª•ÈÄöËøá |p1' - p2'| ËÆ°ÁÆóÔºàÊØîÂ¶ÇÁ¨¨ 10000 ‰∏™ token ÁöÑ p1'=0ÔºåÁ¨¨ 10001 ‰∏™ token ÁöÑ p2'=1ÔºåÁõ∏ÂØπ‰ΩçÁΩÆÊòØ 1ÔºåÂç≥ ‚ÄúÁõ∏ÈÇª‚ÄùÔºâ„ÄÇ‚úÖ ÁîüÊ¥ªÂØπÂ∫îÔºöÁÆ°ÁêÜÂëòÈÄöËøá p' ÁöÑÂ∑ÆÂÄºÔºåÁõ¥Êé•Áü•ÈÅì ‚Äú‰∏§Êú¨‰π¶ÁöÑÂâçÂêéÂÖ≥Á≥ª‚ÄùÔºå‰∏çÁî®ÁÆ°ÂÆÉ‰ª¨Âú®Âì™‰∏™Âë®Êúü„ÄÇ

Á¨¨‰∫åÊ≠•ÔºöÂä®ÊÄÅ‰∏ä‰∏ãÊñáÂéãÁº©
Ôºà1ÔºâÂéãÁº©ÂÖ¨Âºè
ÊääL‰∏™tokenÔºåÂàÜÊàêMÁªÑÔºåÊØèÁªÑÂæóÂà∞ÊØè‰∏™tokenÁöÑÈáçË¶ÅÊÄßÔºåÂä†ÊùÉÊ±ÇÂíåÔºåÂæóÂà∞ÊØè‰∏™‰ª£Ë°®ÊÄßÁöÑ‰ª£Ë°®ÊÄßÔºàÂÖ±M‰∏™Ôºâ
L - M = NÔºå ‰ºöÂú®Â¢ûÂä†N‰∏™
--------------------
YARN ‰ºöÈÄöËøá„ÄåÊ≥®ÊÑèÂäõÊ±†ÂåñÔºàAttention PoolingÔºâ„ÄçÂØπÊó©Êúü‰∏ä‰∏ãÊñáËøõË°åÂéãÁº©ÔºåÂæóÂà∞¬†M¬†‰∏™Ê†∏ÂøÉËØ≠‰πâÂêëÈáè¬†H_compressedÔºö\(H_{compressed} = \text{AttentionPool}(H, W) = \sum_{i=1}^L \alpha_i \cdot h_i\)ÊãÜËß£ 1ÔºöŒ±_i = softmax(W ¬∑ h_i)ÔºàÊ≥®ÊÑèÂäõÊùÉÈáçÔºâ‚Üí¬†Œ±_i¬†ÊòØÁ¨¨¬†i¬†‰∏™ token ÁöÑ ‚ÄúÈáçË¶ÅÊÄßÂæóÂàÜ‚ÄùÔºåÊÄªÂíå‰∏∫ 1ÔºàÊØîÂ¶ÇÈáçË¶ÅÁöÑ token ÂæóÂàÜ¬†Œ±_i=0.01ÔºåÂÜó‰ΩôÁöÑ token ÂæóÂàÜ¬†Œ±_i=0.0001ÔºâÔºõ
‚úÖ ÁîüÊ¥ªÂØπÂ∫îÔºöÁÆ°ÁêÜÂëòÂà§Êñ≠ ‚ÄúÁ¨¨ 3 Êú¨‰π¶ËÆ≤Ê†∏ÂøÉÂéüÁêÜÔºàŒ±_i È´òÔºâÔºåÁ¨¨ 5 Êú¨‰π¶ÊòØÈáçÂ§ç‰∏æ‰æãÔºàŒ±_i ‰ΩéÔºâ‚ÄùÔºåÈáçÁÇπËÆ∞ÂâçËÄÖ„ÄÇ

ÊãÜËß£ 2Ôºösum(Œ±_i ¬∑ h_i)¬†‚Üí Âä†ÊùÉÊ±ÇÂíåÔºåÊää¬†L¬†‰∏™ËØ≠‰πâÂêëÈáè ‚ÄúÊµìÁº©‚Äù Êàê¬†M¬†‰∏™ÔºàYARN ‰ºöÂàÜ¬†M¬†ÁªÑËÆ°ÁÆóÔºåÊØèÁªÑÂØπÂ∫î‰∏Ä‰∏™Ê†∏ÂøÉÂêëÈáèÔºâÔºåÁõ∏ÂΩì‰∫é ‚ÄúÊää 1000 Êú¨‰π¶ÁöÑÈáçÁÇπÔºåÊèêÁÇºÊàê 1 Êù°Á¨îËÆ∞‚ÄùÔºõ
‚úÖ ÁîüÊ¥ªÂØπÂ∫îÔºöÁÆ°ÁêÜÂëò‰∏çËÆ∞ÊØèÊú¨‰π¶ÁöÑÈÄêÂ≠óÂÜÖÂÆπÔºåÂè™ËÆ∞ ‚ÄúËøô 1000 Êú¨‰π¶ÁöÑÊ†∏ÂøÉÊòØ XX‚ÄùÔºåÁ¨îËÆ∞‰ΩìÁßØÂ∞èÔºàÂç†Á©∫Èó¥Â∞ëÔºâ‰ΩÜ‰ø°ÊÅØÂØÜÂ∫¶È´ò„ÄÇ

Ôºà2ÔºâÁ™óÂè£Êõ¥Êñ∞ÈÄªËæëÔºàÂÖ¨ÂºèÈöêÂê´ÊµÅÁ®ãÔºâÂΩìÊñ∞ÁöÑ¬†N¬†‰∏™ token ËøõÊù•ÔºàÊØîÂ¶ÇÊñ∞ÁöÑ 1000 Êú¨‰π¶ÔºâÔºåÁ™óÂè£Êõ¥Êñ∞ÂÖ¨Âºè‰∏∫Ôºö\(H_{new} = [H_{compressed}, h_{L+1}, h_{L+2}, ..., h_{L+N}]\)ÊãÜËß£ÔºöÊääÂéãÁº©ÂêéÁöÑ¬†M¬†‰∏™Ê†∏ÂøÉÂêëÈáèÔºàÁ¨îËÆ∞Êú¨Á¨îËÆ∞ÔºâÔºåÂíåÊñ∞ÁöÑ¬†N¬†‰∏™ token ËØ≠‰πâÂêëÈáèÔºàÊñ∞ÊëÜ‰∏äÊ°åÈù¢ÁöÑ‰π¶ÔºâÊãºÊé•ÔºåÊÄªÈïøÂ∫¶‰ªç‰∏∫¬†LÔºàM + N = LÔºâÔºåÊó¢Ê≤°Ë∂ÖÂá∫Á™óÂè£ÔºåÂèà‰øùÁïô‰∫ÜÂâçÊñáÈáçÁÇπÔºõ
‚úÖ ÁîüÊ¥ªÂØπÂ∫îÔºöÊ°åÈù¢ÂßãÁªà‰øùÊåÅ 2000 ‰∏™ ‚Äú‰ø°ÊÅØÂçïÂÖÉ‚ÄùÔºà1000 Êù°Á¨îËÆ∞ + 1000 Êú¨Êñ∞‰π¶ÔºâÔºå‰∏ç‰ºöÊ∫¢Âá∫Ôºå‰∏îÁ¨îËÆ∞ËÉΩÊõø‰ª£Âéü‰π¶ÁöÑÊ†∏ÂøÉ‰ø°ÊÅØ„ÄÇ


Yarn-->RoPE->embedding-->word2verc
‰∏∫‰∫ÜËÆ©Ê®°ÂûãÂ§ÑÁêÜÊñáÊú¨ÔºåÈúÄË¶ÅÂ∞ÜÊñáÊú¨Êï∞ÂÄºÂåñÔºåÈô§‰∫Üone-hotËøôÁßçÊñπÂºèÂ§ñÔºå‰ΩÜËøô‰ºöÊúâÂîØÁã¨ÁÅæÈöæÔºàÊúâÂ§ßÈáèÁöÑÊó†ÊïàÁöÑ0Ôºå‰∏î‰∏çËÉΩË°®Á§∫ËØ≠‰πâÔºâÔºå‰ΩÜÂ¶ÇÊûúÁî®‰∏ÄÁßçÁ®†ÂØÜÂêëÈáèË°®Á§∫Âçï‰∏™ËØçËØ≠(token)ÔºåËÆ©Áõ∏‰ººËØ≠‰πâÁöÑËØçËØ≠(token)ÂêëÈáèÂ§πËßíËæÉÂ∞èÔºåÁõ∏ÂêåËØ≠‰πâÔºàËØ≠‰πâËæÉÂº∫ÁöÑÔºåÊ®°ÈïøÁöÑÂ§ßÔºåËØ≠‰πâÂ∞èÁöÑÔºåÊ®°ÈïøÂ∞èÔºâÔºå
‰ΩÜÊÄé‰πàÂÅöÂà∞Âë¢ÔºüÊñπÊ≥ïÂ∞±ÊòØ‰ΩøÁî®ÂØπÊØîÊçüÂ§±ÂáΩÊï∞ÔºåËÆ≠ÁªÉÊ®°ÂûãÔºåÂº∫Ëø´Ê®°ÂûãÂ∞ÜËØ≠‰πâÁõ∏‰ººÁöÑÔºåÂêëÈáèÂ§πËßíÁõ∏Ëøë„ÄÇÂÖ∑‰ΩìËøáÁ®ãÂ¶Ç‰∏ãÔºö
Êàë‰ª¨Áî® SimCSE ËÆ≠ÁªÉÂè•ÂêëÈáè ÁöÑÂú∫ÊôØÂÅöÂÆûÈôÖËÆ≠ÁªÉ‰æãÂ≠ê ‚Äî‚Äî ËøôÊòØÂØπÊØîÂ≠¶‰π†ÔºàInfoNCE LossÔºâÊúÄÁªèÂÖ∏ÁöÑÂ∫îÁî®ÔºåÂÖ®Á®ãËøòÂéü ‚ÄúÊï∞ÊçÆÂáÜÂ§á‚ÜíÊ®°ÂûãËÆ°ÁÆó‚ÜíÊçüÂ§±‰ºòÂåñ‚ÜíÂêëÈáèÊî∂Êïõ‚Äù ÁöÑÂÆåÊï¥ËøáÁ®ãÔºåÊØè‰∏™Ê≠•È™§ÈÉΩÂØπÂ∫îÂÖ¨ÂºèÔºåÁõ¥ËßÇÁúãÂà∞ÊçüÂ§±ÂáΩÊï∞Â¶Ç‰Ωï ‚ÄúÈÄºÁùÄ‚Äù ÂêëÈáèÊª°Ë∂≥ËØ≠‰πâÁ∫¶Êùü„ÄÇ
ËÆ≠ÁªÉÂÆåÊØïÂêéÔºåÁõ∏ÂΩì‰∫éÁõ∏ÂêåËØ≠‰πâÁöÑÔºåÂêëÈáèÂú®Âêå‰∏ÄÈôÑËøëÔºåÂÆûÈôÖË∑ü‰∫∫ÁöÑÊÄùÊÉ≥ÊúâÁÇπÂÉèÔºåË∞à‰∏Ä‰∏™ËØùÈ¢òÔºåÁõ∏ÂêåËØ≠‰πâÁöÑÂ§ö‰∏™ËØçÔºåÁõ∏ÁªßÂá∫Áé∞ÁöÑÊ¶ÇÁéáÂ§ß„ÄÇ
Ê®°Âûã‰∏çÁêÜËß£Áé∞ÂÆûÁöÑÊÑèÊÄùÔºå‰ªñÂè™ÊòØÊòéÁôΩÊ†πÊçÆÂâçÈù¢ÁöÑÂ§ö‰∏™ÂêëÈáèÔºåÂêëÈáèÁöÑÈ°∫Â∫èÔºå‰∏ã‰∏Ä‰∏™ÂêëÈáèÂ∫îËØ•ÊòØËøô‰∏™ÔºåÁÑ∂ÂêéËæìÂá∫ÔºåËΩ¨Êç¢‰∏∫‰∫∫ÁêÜËß£ÁöÑËØçËØ≠ÔºåÁúãÁùÄÂ§ßÊ®°Âûã‰ºº‰πéÁêÜËß£‰∫ÜÔºåÊàëÁöÑÊÑèÊÄùÔºåÂÆûÈôÖ‰∏ä‰∏çÊòØÔºåÂè™ÊòØÈÇ£‰∏™ÂêëÈáèË¢´ËÆ°ÁÆóÂá∫ÊòØ‰∏ã‰∏Ä‰∏™ÂêëÈáèÁöÑÊ¶ÇÁéáÂ§ß„ÄÇ

ËÆ≠ÁªÉËøáÁ®ãÔºö12



‰∏Ä„ÄÅËÆ≠ÁªÉ‰ªªÂä°ÂÆö‰πâ
ÁõÆÊ†áÔºöËÆ≠ÁªÉÊ®°ÂûãËÆ©„ÄåÂêå‰πâÂè•ÂêëÈáèÂ§πËßíÊé•Ëøë 0¬∞ÔºåÈùûÂêå‰πâÂè•ÂêëÈáèÂ§πËßíÊé•Ëøë 180¬∞„Äç„ÄÇÈÄâÁî® 3 ‰∏™Âè•Â≠ê‰Ωú‰∏∫ËÆ≠ÁªÉÊ†∑Êú¨ÔºàÊ®°ÊãüÊµ∑ÈáèËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑ‰∏Ä‰∏™ÊâπÊ¨°ÔºâÔºö
ÈîöÁÇπÂè•ÔºàxÔºâÔºö‚ÄúÊàëÁà±ÂêÉËãπÊûú‚ÄùÔºàÊ†∏ÂøÉËØ≠‰πâÔºöËãπÊûúÁõ∏ÂÖ≥Ôºâ
Ê≠£Ê†∑Êú¨Âè•Ôºàx‚Å∫ÔºâÔºö‚ÄúÊàëÂñúÊ¨¢ÂêÉËãπÊûú‚ÄùÔºàÂíåÈîöÁÇπÂêå‰πâÔºåËØ≠‰πâÁõ∏‰ººÔºâ
Ë¥üÊ†∑Êú¨Âè•Ôºàx‚Åª‚ÇÅ, x‚Åª‚ÇÇÔºâÔºö‚ÄúÊàëÁà±ÂêÉÊ±ΩËΩ¶‚Äù‚ÄúÁü≥Â§¥ÂæàÁ°¨‚ÄùÔºàÂíåÈîöÁÇπ‰∏çÂêå‰πâÔºåËØ≠‰πâ‰∏çÁõ∏‰ººÔºâ
Ê®°ÂûãÔºöÁî®ÁÆÄÂåñÁâà BERTÔºà‰ªÖ‰øùÁïôÁºñÁ†ÅÂô®ÂíåÂè•ÂêëÈáèËæìÂá∫Â±ÇÔºâÔºåÂè•ÂêëÈáèÁª¥Â∫¶‰∏∫ 2ÔºàÊñπ‰æøËÆ°ÁÆóÂíåÂèØËßÜÂåñÔºåÂÆûÈôÖÊòØ 768 Áª¥ÔºåÈÄªËæëÂÆåÂÖ®‰∏ÄËá¥Ôºâ„ÄÇË∂ÖÂèÇÊï∞ÔºöÊ∏©Â∫¶ œÑ=0.1ÔºàÊéßÂà∂Âå∫ÂàÜÂ∫¶Ôºå‰∏ªÊµÅÂèñÂÄºÔºâ„ÄÇ
‰∫å„ÄÅStep1ÔºöÊï∞ÊçÆÂáÜÂ§á‰∏éÂêëÈáèÂàùÂßãÂåñ
ËÆ≠ÁªÉÂàöÂºÄÂßãÊó∂ÔºåÊ®°ÂûãÁöÑÂè•ÂêëÈáèÊòØ ÈöèÊú∫ÂàùÂßãÂåñ ÁöÑÔºàÂÆåÂÖ®‰∏çÁ¨¶ÂêàËØ≠‰πâÔºâÔºåÊàë‰ª¨ÂÖàËÆ∞ÂΩïÂàùÂßãÂêëÈáèÔºàÈöèÊú∫ÁîüÊàêÂêàÁêÜËåÉÂõ¥ÁöÑÊï∞ÂÄºÔºâÔºö
ÈîöÁÇπÂêëÈáè x = [0.2, 0.3]·µÄÔºàÈïøÂ∫¶‚à•x‚à•=‚àö(0.2¬≤+0.3¬≤)‚âà0.36Ôºâ
Ê≠£Ê†∑Êú¨ÂêëÈáè x‚Å∫ = [0.5, 0.1]·µÄÔºàÈïøÂ∫¶‚à•x‚Å∫‚à•=‚àö(0.5¬≤+0.1¬≤)‚âà0.51Ôºâ
Ë¥üÊ†∑Êú¨ÂêëÈáè x‚Åª‚ÇÅ = [0.7, 0.8]·µÄÔºàÈïøÂ∫¶‚à•x‚Åª‚ÇÅ‚à•=‚àö(0.7¬≤+0.8¬≤)‚âà1.06Ôºâ
Ë¥üÊ†∑Êú¨ÂêëÈáè x‚Åª‚ÇÇ = [0.1, 0.9]·µÄÔºàÈïøÂ∫¶‚à•x‚Åª‚ÇÇ‚à•=‚àö(0.1¬≤+0.9¬≤)‚âà0.91Ôºâ
Ê≠§Êó∂ÂêëÈáèÂÆåÂÖ®Ê∑∑‰π±ÔºöÊØîÂ¶ÇÊ≠£Ê†∑Êú¨ x‚Å∫ÂíåÈîöÁÇπ x ÁöÑÂ§πËßíÂæàÂ§ßÔºåË¥üÊ†∑Êú¨ x‚Åª‚ÇÅÂíå x ÁöÑÂ§πËßíÂæàÂ∞è ‚Äî‚Äî ÊçüÂ§±ÂáΩÊï∞‰ºöÊçïÊçâÂà∞ËøôÁßç ‚ÄúËØ≠‰πâ‰∏çÂåπÈÖç‚ÄùÔºåÂπ∂Ëß¶Âèë‰ºòÂåñ„ÄÇ
‰∏â„ÄÅStep2ÔºöËÆ°ÁÆó InfoNCE LossÔºàÊ†∏ÂøÉÂÖ¨ÂºèÂ∫îÁî®Ôºâ
Ê†πÊçÆ InfoNCE Loss ÂÖ¨ÂºèÔºåÂàÜ 3 Ê≠•ËÆ°ÁÆóÊçüÂ§±Ôºö
1. ËÆ°ÁÆóÊâÄÊúâÊ†∑Êú¨ÂØπÁöÑÂ§πËßí‰ΩôÂº¶ÂÄºÔºàcosŒ∏Ôºâ
ÂÖ≥ÈîÆÂÖ¨ÂºèÔºöcosŒ∏‚Çì·µß = (x„Éªy)/(‚à•x‚à•„Éª‚à•y‚à•)ÔºàÁÇπÁßØ √∑ ÈïøÂ∫¶‰πòÁßØÔºâ
Ê≠£Ê†∑Êú¨ÂØπÔºàx, x‚Å∫ÔºâÔºöx„Éªx‚Å∫ = 0.2√ó0.5 + 0.3√ó0.1 = 0.1 + 0.03 = 0.13cosŒ∏‚Çì‚Çì‚Å∫ = 0.13/(0.36√ó0.51) ‚âà 0.13/0.18 ‚âà 0.722ÔºàÂ§πËßí‚âà43¬∞ÔºåÂ§™Â§ßÔºå‰∏çÁ¨¶Âêà ‚ÄúÂêå‰πâÂè•Ëøë‚ÄùÔºâ
Ë¥üÊ†∑Êú¨ÂØπÔºàx, x‚Åª‚ÇÅÔºâÔºöx„Éªx‚Åª‚ÇÅ = 0.2√ó0.7 + 0.3√ó0.8 = 0.14 + 0.24 = 0.38cosŒ∏‚Çì‚Çì‚Åª¬π = 0.38/(0.36√ó1.06) ‚âà 0.38/0.38 ‚âà 1.0ÔºàÂ§πËßí‚âà0¬∞ÔºåÂ§™Â∞èÔºå‰∏çÁ¨¶Âêà ‚ÄúÈùûÂêå‰πâÂè•Ëøú‚ÄùÔºâ
Ë¥üÊ†∑Êú¨ÂØπÔºàx, x‚Åª‚ÇÇÔºâÔºöx„Éªx‚Åª‚ÇÇ = 0.2√ó0.1 + 0.3√ó0.9 = 0.02 + 0.27 = 0.29cosŒ∏‚Çì‚Çì‚Åª¬≤ = 0.29/(0.36√ó0.91) ‚âà 0.29/0.33 ‚âà 0.879ÔºàÂ§πËßí‚âà28¬∞ÔºåÂ§™Â∞èÔºå‰∏çÁ¨¶ÂêàÁ∫¶ÊùüÔºâ
2. ËÆ°ÁÆóÂàÜÂ≠êÂíåÂàÜÊØçÔºàÂÖ¨ÂºèÊ†∏ÂøÉÈ°πÔºâ
ÂàÜÂ≠êÔºöexp (cosŒ∏‚Çì‚Çì‚Å∫ / œÑ) = exp (0.722 / 0.1) = exp (7.22) ‚âà 1360ÂàÜÊØçÔºöÂàÜÂ≠ê + sum (exp (cosŒ∏‚Çì‚Çì‚Åª·µ¢ / œÑ)) = 1360 + exp (1.0/0.1) + exp (0.879/0.1)= 1360 + exp(10) + exp(8.79) ‚âà 1360 + 22026 + 7350 ‚âà 30736
3. ËÆ°ÁÆóÊúÄÁªàÊçüÂ§±
L = -log (ÂàÜÂ≠ê / ÂàÜÊØç) = -log (1360/30736) ‚âà -log (0.044) ‚âà 3.13ÔºàÊçüÂ§±ÂÄºÂæàÂ§ßÔºåËØ¥ÊòéÂêëÈáè‰∏•Èáç‰∏çÁ¨¶ÂêàËØ≠‰πâÁ∫¶ÊùüÔºâ
Âõõ„ÄÅStep3ÔºöÂèçÂêë‰º†Êí≠‰ºòÂåñÔºàÊçüÂ§±ÂáΩÊï∞ÈÄºÁùÄÂêëÈáèË∞ÉÊï¥Ôºâ
Ê®°ÂûãÁöÑÁõÆÊ†áÊòØ ‚ÄúÊúÄÂ∞èÂåñÊçüÂ§± L‚ÄùÔºåÈÄöËøá ÂèçÂêë‰º†Êí≠ Ë∞ÉÊï¥ÂêëÈáèÁöÑÊØè‰∏™ÂàÜÈáèÔºà0.2„ÄÅ0.3„ÄÅ0.5 Á≠âÊï∞ÂÄºÔºâÔºåË∞ÉÊï¥ÊñπÂêëÂÆåÂÖ®Áî± InfoNCE Loss ÁöÑÊ¢ØÂ∫¶ÂÜ≥ÂÆöÔºö
ÂØπÊ≠£Ê†∑Êú¨ x‚Å∫ÔºöË¶ÅËÆ© cosŒ∏‚Çì‚Çì‚Å∫Â¢ûÂ§ßÔºàÊé•Ëøë 1Ôºâ‚Üí Ë∞ÉÊï¥ x‚Å∫ÁöÑÂàÜÈáèÔºåËÆ©ÂÆÉÂíå x ÁöÑÊñπÂêëÊõ¥Êé•ËøëÔºàÊØîÂ¶Ç x‚Å∫‰ªé [0.5,0.1]‚Üí[0.3,0.4]ÔºåÂíå x=[0.2,0.3] ÊñπÂêëË∂ãÂêåÔºâÔºõ
ÂØπË¥üÊ†∑Êú¨ x‚Åª‚ÇÅ„ÄÅx‚Åª‚ÇÇÔºöË¶ÅËÆ© cosŒ∏‚Çì‚Çì‚Åª·µ¢ÂáèÂ∞èÔºàÊé•Ëøë - 1Ôºâ‚Üí Ë∞ÉÊï¥ x‚Åª‚ÇÅ„ÄÅx‚Åª‚ÇÇÁöÑÂàÜÈáèÔºåËÆ©ÂÆÉ‰ª¨Âíå x ÁöÑÊñπÂêëÁõ∏ÂèçÔºàÊØîÂ¶Ç x‚Åª‚ÇÅ‰ªé [0.7,0.8]‚Üí[-0.3,-0.4]ÔºåÂíå x ÊñπÂêëÁõ∏ÂèçÔºâÔºõ
ÂØπÈîöÁÇπ xÔºöÂæÆË∞ÉÂàÜÈáèÔºåËÆ©ÂÆÉÂíå x‚Å∫ÁöÑÊñπÂêëÊõ¥‰∏ÄËá¥ÔºåÂêåÊó∂Âíå x‚Åª‚ÇÅ„ÄÅx‚Åª‚ÇÇÁöÑÊñπÂêëÊõ¥Áõ∏Âèç„ÄÇ
Ëøô‰∏™ËøáÁ®ã‰ºö ÂèçÂ§çËø≠‰ª£ÔºàÊØîÂ¶ÇËÆ≠ÁªÉ 1000 ËΩÆÔºâÔºåÊØè‰∏ÄËΩÆÈÉΩÈáçÊñ∞ËÆ°ÁÆóÊçüÂ§±„ÄÅË∞ÉÊï¥ÂêëÈáèÔºåÁõ¥Âà∞ÊçüÂ§±ÈôçÂà∞ÊúÄ‰Ωé„ÄÇ
‰∫î„ÄÅStep4ÔºöËÆ≠ÁªÉÊî∂ÊïõÔºàÂêëÈáèÊª°Ë∂≥ËØ≠‰πâÁ∫¶ÊùüÔºâ
ÁªèËøáÂ§öËΩÆËø≠‰ª£ÂêéÔºåÊçüÂ§± L ‰ªé 3.13 ÈôçÂà∞ 0.01ÔºàÊé•ËøëÊúÄÂ∞èÂÄºÔºâÔºåÊ≠§Êó∂ÁöÑÂêëÈáèÂÆåÂÖ®Á¨¶ÂêàËØ≠‰πâÈÄªËæëÔºö
ÈîöÁÇπÂêëÈáè x = [3, 4]·µÄÔºàÈïøÂ∫¶‚à•x‚à•=5ÔºåÊ†∏ÂøÉËØ≠‰πâÔºöËãπÊûúÔºâ
Ê≠£Ê†∑Êú¨ÂêëÈáè x‚Å∫ = [6, 8]·µÄÔºàÈïøÂ∫¶‚à•x‚Å∫‚à•=10ÔºåÊòØ x ÁöÑ 2 ÂÄçÔºåÊñπÂêëÂÆåÂÖ®Áõ∏ÂêåÔºâ
Ë¥üÊ†∑Êú¨ÂêëÈáè x‚Åª‚ÇÅ = [-3, -4]·µÄÔºàÈïøÂ∫¶‚à•x‚Åª‚ÇÅ‚à•=5ÔºåÂíå x ÊñπÂêëÂÆåÂÖ®Áõ∏ÂèçÔºâ
Ë¥üÊ†∑Êú¨ÂêëÈáè x‚Åª‚ÇÇ = [-6, -8]·µÄÔºàÈïøÂ∫¶‚à•x‚Åª‚ÇÇ‚à•=10ÔºåÂíå x ÊñπÂêëÂÆåÂÖ®Áõ∏ÂèçÔºâ
È™åËØÅÔºöÈáçÊñ∞ËÆ°ÁÆóÊçüÂ§±ÔºàÁ¨¶ÂêàÁ∫¶ÊùüÔºâ
ËÆ°ÁÆó cosŒ∏Ôºö
cosŒ∏‚Çì‚Çì‚Å∫ = (3√ó6 + 4√ó8)/(5√ó10) = (18+32)/50 = 50/50 = 1.0ÔºàÂ§πËßí 0¬∞ÔºåÂêå‰πâÂè•ËøëÔºâ
cosŒ∏‚Çì‚Çì‚Åª¬π = (3√ó(-3) + 4√ó(-4))/(5√ó5) = (-9-16)/25 = -25/25 = -1.0ÔºàÂ§πËßí 180¬∞ÔºåÈùûÂêå‰πâÂè•ËøúÔºâ
cosŒ∏‚Çì‚Çì‚Åª¬≤ = (3√ó(-6) + 4√ó(-8))/(5√ó10) = (-18-32)/50 = -50/50 = -1.0ÔºàÂ§πËßí 180¬∞ÔºåÁ¨¶ÂêàÁ∫¶ÊùüÔºâ
ËÆ°ÁÆóÊçüÂ§±ÔºöÂàÜÂ≠ê = exp (1.0/0.1) = exp (10) ‚âà 22026ÂàÜÊØç = 22026 + exp (-1.0/0.1) + exp (-1.0/0.1) = 22026 + 2√óexp (-10) ‚âà 22026Ôºàexp (-10)‚âà4.5e-5ÔºåÂèØÂøΩÁï•ÔºâL = -log (22026/22026) = -log (1) = 0ÔºàÊçüÂ§±ÊúÄÂ∞èÔºåÂêëÈáèÂÆåÂÖ®Êª°Ë∂≥ËØ≠‰πâÁ∫¶ÊùüÔºâ
ÂÖ≠„ÄÅËÆ≠ÁªÉÁªìÊûúÁöÑÊ†∏ÂøÉÊÑè‰πâ
ÂêëÈáèÂÖ≥Á≥ªÂåπÈÖçËØ≠‰πâÔºö
Âêå‰πâÂè•Ôºàx Âíå x‚Å∫ÔºâÔºöÊñπÂêëÁõ∏ÂêåÔºàÂ§πËßí 0¬∞ÔºâÔºåÈïøÂ∫¶‰∏çÂêåÔºàx‚Å∫Êõ¥ÈïøÔºå‰ª£Ë°®ËØ≠‰πâÂº∫Â∫¶Êõ¥Âº∫ÔºâÔºõ
ÈùûÂêå‰πâÂè•Ôºàx Âíå x‚Åª‚ÇÅ„ÄÅx‚Åª‚ÇÇÔºâÔºöÊñπÂêëÁõ∏ÂèçÔºàÂ§πËßí 180¬∞ÔºâÔºåÈïøÂ∫¶‰∏çÂΩ±ÂìçËØ≠‰πâÂ∑ÆÂºÇ„ÄÇ
ÊçüÂ§±ÂáΩÊï∞ÁöÑ‰ΩúÁî®Ôºö
Êï¥‰∏™ËøáÁ®ã‰∏≠ÔºåInfoNCE Loss ÊòØ ‚ÄúÊåáÊå•Ê£í‚Äù‚Äî‚Äî ÈÄöËøá ‚ÄúÊÉ©ÁΩö‰∏çÁ¨¶ÂêàËØ≠‰πâÁöÑÂêëÈáèÂÖ≥Á≥ª‚ÄùÔºàÂàùÂßãÊçüÂ§±Â§ßÔºâÔºåÈÄºÁùÄÊ®°ÂûãË∞ÉÊï¥ÂêëÈáèÔºåÊúÄÁªàËÆ© ‚ÄúÂêëÈáèÂ§πËßí‚Äù ÂÆåÁæéÂåπÈÖç ‚ÄúËØ≠‰πâÁõ∏‰ººÂ∫¶‚Äù„ÄÇ
ÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÔºö
ËÆ≠ÁªÉÂ•ΩÂêéÔºåÁªôÊ®°ÂûãËæìÂÖ• ‚ÄúÊàëÁà±ÂêÉÁ∫¢ËãπÊûú‚ÄùÔºàÊñ∞ÁöÑËãπÊûúÁõ∏ÂÖ≥Âè•Â≠êÔºâÔºåÂÆÉ‰ºöËæìÂá∫Âíå x ÊñπÂêëÊé•ËøëÁöÑÂêëÈáèÔºàÂ§πËßíÂ∞èÔºâÔºõËæìÂÖ• ‚ÄúÁîµËÑëÂæàÂ•ΩÁî®‚ÄùÔºàÊó†ÂÖ≥Âè•Â≠êÔºâÔºå‰ºöËæìÂá∫Âíå x ÊñπÂêëÁõ∏ÂèçÁöÑÂêëÈáèÔºàÂ§πËßíÂ§ßÔºâ‚Äî‚Äî ËøôÂ∞±ÊòØ ‚ÄúÂ§πËßí‰ª£Ë°®ËØ≠‰πâÁõ∏‰ººÂ∫¶‚Äù ÁöÑÊù•Ê∫ê„ÄÇ
ÊÄªÁªìÔºàÂÆûÈôÖËÆ≠ÁªÉÁöÑÊ†∏ÂøÉÈÄªËæëÔºâ
ÂØπÊØîÂ≠¶‰π†ÔºàInfoNCE LossÔºâÁöÑÂÆûÈôÖËÆ≠ÁªÉÔºåÂ∞±ÊòØ ‚ÄúÈöèÊú∫ÂêëÈáè‚ÜíËÆ°ÁÆóÊçüÂ§±ÔºàÊçïÊçâËØ≠‰πâ‰∏çÂåπÈÖçÔºâ‚ÜíÂèçÂêë‰º†Êí≠Ë∞ÉÊï¥ÂêëÈáè‚ÜíÊçüÂ§±ÊúÄÂ∞èÔºàÂêëÈáèÂåπÈÖçËØ≠‰πâÔºâ‚Äù ÁöÑÂæ™ÁéØ„ÄÇÊàë‰ª¨‰∏æÁöÑ 2 Áª¥ÂêëÈáè‰æãÂ≠êÔºåÂíåÂÆûÈôÖÂ§ßÊ®°Âûã 768 Áª¥ÂêëÈáèÁöÑËÆ≠ÁªÉÈÄªËæëÂÆåÂÖ®‰∏ÄËá¥ ‚Äî‚Äî ÊçüÂ§±ÂáΩÊï∞ÈÄöËøáÊï∞Â≠¶Á∫¶ÊùüÔºåÊää ‚ÄúÂêå‰πâËøë„ÄÅÈùûÂêå‰πâËøú‚Äù ÁöÑËØ≠‰πâÈÄªËæëÔºåÂàªËøõ‰∫ÜÂêëÈáèÁöÑÂá†‰ΩïÂÖ≥Á≥ªÈáå„ÄÇ
'''
def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), rope_base: float = 1e6,
                         rope_scaling: Optional[dict] = None):
    freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # # yarnÈïøÂ∫¶Â§ñÊé®
    if rope_scaling is not None:
        orig_max, factor, beta_fast, beta_slow = (
            rope_scaling.get("original_max_position_embeddings", 2048), rope_scaling.get("factor", 4),
            rope_scaling.get("beta_fast", 4.0), rope_scaling.get("beta_slow", 1.0)
        )
        
        if end / orig_max > 1.0:
            corr_dim = next((i for i in range(dim // 2) if 2 * math.pi / freqs[i] > orig_max), dim // 2)
            power = torch.arange(0, dim // 2, device=freqs.device).float() / max(dim // 2 - 1, 1)
            beta = beta_slow + (beta_fast - beta_slow) * power
            # Œª = (Œ≤¬∑Œ± - Œ≤ + 1)/(Œ≤¬∑Œ±) YaRNÊ†áÂáÜÂÖ¨Âºè
            scale = torch.where(torch.arange(dim // 2, device=freqs.device) < corr_dim, (beta * factor - beta + 1) / (beta * factor), 1.0 / factor)
            freqs = freqs * scale

    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float()
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)
    return freqs_cos, freqs_sin


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    def rotate_half(x):
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        x[:, :, :, None, :].expand(bs, slen, num_key_value_heads, n_rep, head_dim).reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )


class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        assert args.num_attention_heads % self.num_key_value_heads == 0
        self.n_local_heads = args.num_attention_heads
        self.n_local_kv_heads = self.num_key_value_heads
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        self.head_dim = args.hidden_size // args.num_attention_heads
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
        # print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(self,
                x: torch.Tensor,
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # ‰øÆÊîπ‰∏∫Êé•Êî∂cosÂíåsin
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache=False,
                attention_mask: Optional[torch.Tensor] = None):
        bsz, seq_len, _ = x.shape
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)

        cos, sin = position_embeddings
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # kv_cacheÂÆûÁé∞
        if past_key_value is not None:
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        past_kv = (xk, xv) if use_cache else None

        xq, xk, xv = (
            xq.transpose(1, 2),
            repeat_kv(xk, self.n_rep).transpose(1, 2),
            repeat_kv(xv, self.n_rep).transpose(1, 2)
        )

        if self.flash and seq_len > 1 and (attention_mask is None or torch.all(attention_mask == 1)):
            attn_mask = (
                None
                if attention_mask is None
                else attention_mask.view(bsz, 1, 1, -1).expand(bsz, self.n_local_heads, seq_len, -1).bool()
            )

            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=self.dropout if self.training else 0.0, is_causal=True)
        else:
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            scores = scores + torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1
            ).unsqueeze(0).unsqueeze(0)  # scores+mask

            if attention_mask is not None:
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask

            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            output = scores @ xv

        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        output = self.resid_dropout(self.o_proj(output))
        return output, past_kv


class FeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 / 3)
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.dropout = nn.Dropout(config.dropout)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))


class MoEGate(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.n_routed_experts = config.n_routed_experts

        self.scoring_func = config.scoring_func
        self.alpha = config.aux_loss_alpha
        self.seq_aux = config.seq_aux

        self.norm_topk_prob = config.norm_topk_prob
        self.gating_dim = config.hidden_size
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        self.reset_parameters()

    def reset_parameters(self) -> None:
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        bsz, seq_len, h = hidden_states.shape
        hidden_states = hidden_states.view(-1, h)
        logits = F.linear(hidden_states, self.weight, None)
        if self.scoring_func == 'softmax':
            scores = logits.softmax(dim=-1)
        else:
            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')

        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)

        if self.top_k > 1 and self.norm_topk_prob:
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            topk_weight = topk_weight / denominator

        if self.training and self.alpha > 0.0:
            scores_for_aux = scores
            aux_topk = self.top_k
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1)
            if self.seq_aux:
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                ce.scatter_add_(1, topk_idx_for_aux_loss,
                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(
                    seq_len * aux_topk / self.n_routed_experts)
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                ce = mask_ce.float().mean(0)
                Pi = scores_for_aux.mean(0)
                fi = ce * self.n_routed_experts
                aux_loss = (Pi * fi).sum() * self.alpha
        else:
            aux_loss = 0
        return topk_idx, topk_weight, aux_loss


class MOEFeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.experts = nn.ModuleList([
            FeedForward(config)
            for _ in range(config.n_routed_experts)
        ])
        self.gate = MoEGate(config)
        if config.n_shared_experts > 0:
            self.shared_experts = nn.ModuleList([
                FeedForward(config)
                for _ in range(config.n_shared_experts)
            ])

    def forward(self, x):
        identity = x
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        # ‰ΩøÁî®Èó®ÊéßÊú∫Âà∂ÈÄâÊã©‰∏ìÂÆ∂
        topk_idx, topk_weight, aux_loss = self.gate(x)
        x = x.view(-1, x.shape[-1])
        flat_topk_idx = topk_idx.view(-1)
        if self.training:
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
            y = torch.empty_like(x, dtype=torch.float16)
            for i, expert in enumerate(self.experts):
                y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to(y.dtype)  # Á°Æ‰øùÁ±ªÂûã‰∏ÄËá¥
            y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1)
            y = y.view(*orig_shape)
        else:
            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity)
        self.aux_loss = aux_loss
        return y

    @torch.no_grad()
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        expert_cache = torch.zeros_like(x)
        idxs = flat_expert_indices.argsort()
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        token_idxs = idxs // self.config.num_experts_per_tok
        # ÂΩìtokens_per_expert = [6, 15, 20, 26]Ôºåtokens_per_expert.shape[0]Âç≥‰∏∫‰∏ìÂÆ∂Êï∞ÈáèÔºàÊ≠§Êó∂‰∏∫4Ôºâ
        # ‰∏îtoken_idxs = [3, 7, 19, 21, 24, 25,  4,  5,  6, 10, 11, 12...] Êó∂
        # ÊÑèÂë≥token_idxs[:6] -> [3, 7, 19, 21, 24, 25]Ëøô6‰∏™‰ΩçÁΩÆÂ±û‰∫é‰∏ìÂÆ∂0Â§ÑÁêÜÁöÑtokenÔºàÊØè‰∏™tokenÊúâÂèØËÉΩË¢´Â§ö‰∏™‰∏ìÂÆ∂Â§ÑÁêÜÔºåËøôÂèñÂÜ≥‰∫énum_experts_per_tokÔºâ
        # Êé•‰∏ãÊù•9‰∏™‰ΩçÁΩÆtoken_idxs[6:15] -> [4,  5,  6, 10, 11, 12...]Â±û‰∫é‰∏ìÂÆ∂1Â§ÑÁêÜÁöÑtoken...‰æùÊ≠§Á±ªÊé®
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]
            if start_idx == end_idx:
                continue
            expert = self.experts[i]
            exp_token_idx = token_idxs[start_idx:end_idx]
            expert_tokens = x[exp_token_idx]
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]])
            expert_cache.scatter_add_(0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out)

        return expert_cache


class MiniMindBlock(nn.Module):
    def __init__(self, layer_id: int, config: MiniMindConfig):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.self_attn = Attention(config)

        self.layer_id = layer_id
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        residual = hidden_states
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states), position_embeddings,
            past_key_value, use_cache, attention_mask
        )
        hidden_states += residual
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        return hidden_states, present_key_value


class MiniMindModel(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.dropout = nn.Dropout(config.dropout)
        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        freqs_cos, freqs_sin = precompute_freqs_cis(dim=config.hidden_size // config.num_attention_heads,
                                                    end=config.max_position_embeddings, rope_base=config.rope_theta,
                                                    rope_scaling=config.rope_scaling)
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                **kwargs):
        batch_size, seq_length = input_ids.shape
        if hasattr(past_key_values, 'layers'): past_key_values = None
        past_key_values = past_key_values or [None] * len(self.layers)
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        hidden_states = self.dropout(self.embed_tokens(input_ids))

        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        presents = []
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)

        hidden_states = self.norm(hidden_states)

        aux_loss = sum(
            layer.mlp.aux_loss
            for layer in self.layers
            if isinstance(layer.mlp, MOEFeedForward)
        )

        return hidden_states, presents, aux_loss


class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    config_class = MiniMindConfig

    def __init__(self, config: MiniMindConfig = None):
        self.config = config or MiniMindConfig()
        super().__init__(self.config)
        self.model = MiniMindModel(self.config)
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        self.model.embed_tokens.weight = self.lm_head.weight
        self.OUT = CausalLMOutputWithPast()

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **args):
        h, past_kvs, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(h[:, slice_indices, :])
        self.OUT.__setitem__('last_hidden_state', h)
        self.OUT.__setitem__('logits', logits)
        self.OUT.__setitem__('aux_loss', aux_loss)
        self.OUT.__setitem__('past_key_values', past_kvs)
        return self.OUT
