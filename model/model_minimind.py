# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

'''
PretrainedConfig æ˜¯æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹é…ç½®ç±»çš„çˆ¶ç±»ï¼Œæä¾›äº†ç»Ÿä¸€çš„æ¥å£ï¼ˆå¦‚ä¿å­˜ / åŠ è½½é…ç½®ã€éªŒè¯å‚æ•°åˆæ³•æ€§ç­‰ï¼‰ï¼Œ
è‡ªå®šä¹‰æ¨¡å‹æ—¶éœ€ç»§æ‰¿æ­¤ç±»ä»¥å…¼å®¹ transformers ç”Ÿæ€ï¼ˆå¦‚ AutoConfig è‡ªåŠ¨åŠ è½½ï¼‰
'''
from transformers import PretrainedConfig

'''
1,åœ¨å½“å‰å·²æœ‰çš„æ‰€æœ‰è¯ï¼ˆè¯­ä¹‰ï¼‰ï¼Œä»¥åŠè¿™äº›è¯çš„å…ˆåé¡ºåºï¼ˆç›¸å¯¹ä½ç½®ï¼‰å…±åŒä½œç”¨ä¸‹ï¼Œä¸‹ä¸€ä¸ªè¯æœ€å¯èƒ½æ˜¯ä»€ä¹ˆ
2,æ¨¡å‹åˆ¤æ–­ â€œä¸‹ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆâ€ æ—¶ï¼Œå¿…é¡»ä¾èµ–ã€Œè¯çš„é¡ºåºã€â€”â€” è€Œ â€œç›¸å¯¹ä½ç½®å…³ç³»â€ å°±æ˜¯æ¨¡å‹è¯†åˆ« â€œé¡ºåºâ€ çš„å”¯ä¸€ä¾æ®ï¼ˆé€šè¿‡ä½ç½®ç¼–ç å®ç°ï¼‰
'''

class MiniMindConfig(PretrainedConfig):
    '''
    transformers çš„ AutoConfig ä¼šé€šè¿‡ model_type è‡ªåŠ¨åŒ¹é…å¯¹åº”çš„é…ç½®ç±»ï¼Œ
    æ˜¯ç”Ÿæ€å…¼å®¹çš„æ ¸å¿ƒæ ‡è¯†ï¼ˆéœ€ä¸æ¨¡å‹æ³¨å†Œæ—¶çš„åç§°ä¸€è‡´ï¼‰
    '''
    model_type = "minimind"

    def __init__(
            self,
            # Dropout æ¦‚ç‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ï¼šæ¨¡å‹ä¸­éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒçš„æ¯”ä¾‹ï¼Œ0.0 è¡¨ç¤ºä¸ä½¿ç”¨ Dropoutã€‚
            dropout: float = 0.0,
            # å¥é¦–æ ‡è®°ï¼ˆBegin Of Sequenceï¼‰çš„ token IDï¼šæ–‡æœ¬ç¼–ç æ—¶ç”¨äºæ ‡è¯†å¥å­å¼€å¤´ï¼Œéœ€ä¸è¯è¡¨ï¼ˆvocabï¼‰ä¸­çš„ ID ä¸€è‡´ã€‚
            bos_token_id: int = 1,
            # å¥å°¾æ ‡è®°ï¼ˆEnd Of Sequenceï¼‰çš„ token IDï¼šæ–‡æœ¬ç¼–ç æ—¶ç”¨äºæ ‡è¯†å¥å­ç»“æŸï¼Œéœ€ä¸è¯è¡¨ä¸€è‡´ã€‚
            eos_token_id: int = 2,
            # éšè—å±‚æ¿€æ´»å‡½æ•°ï¼šsilu å³ Sigmoid Linear Unitï¼ˆÏƒ(x)ãƒ»xï¼‰ï¼Œæ˜¯å¤§æ¨¡å‹å¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼ˆæ¯” ReLU æ›´å¹³æ»‘ï¼‰ï¼Œæ”¯æŒ relu/gelu ç­‰å…¶ä»–é€‰é¡¹ã€‚
            hidden_act: str = 'silu',
            # éšè—å±‚ç»´åº¦ï¼šTransformer ç¼–ç å™¨ / è§£ç å™¨ä¸­æ¯ä¸ª token çš„å‘é‡ç»´åº¦ï¼ˆæ ¸å¿ƒè¶…å‚æ•°ï¼‰ï¼Œå†³å®šæ¨¡å‹å®¹é‡ï¼Œé€šå¸¸ä¸º 2 çš„å¹‚ï¼ˆå¦‚ 512ã€1024ï¼‰ã€‚
            hidden_size: int = 512,
            # Feed-Forward ç½‘ç»œä¸­é—´å±‚ç»´åº¦ï¼šTransformer ä¸­ã€Œè‡ªæ³¨æ„åŠ›å±‚åã€çš„å…¨è¿æ¥å±‚ç»´åº¦ï¼Œé»˜è®¤ None æ—¶é€šå¸¸æŒ‰ hidden_size * 4 è®¡ç®—ï¼ˆå¤§æ¨¡å‹å¸¸è§è®¾è®¡ï¼‰ã€‚
            intermediate_size: int = None,
            # æœ€å¤§åºåˆ—é•¿åº¦ï¼šæ¨¡å‹æ”¯æŒçš„æœ€é•¿è¾“å…¥æ–‡æœ¬é•¿åº¦ï¼ˆtoken æ•°ï¼‰ï¼Œ32768 è¡¨ç¤ºæ”¯æŒ 32k é•¿æ–‡æœ¬ï¼Œéœ€ä¸ä½ç½®ç¼–ç ï¼ˆPositional Embeddingï¼‰çš„ç»´åº¦åŒ¹é…ã€‚
            max_position_embeddings: int = 32768,
            # è‡ªæ³¨æ„åŠ›å¤´æ•°ï¼šå°† hidden_size æ‹†åˆ†ä¸ºå¤šä¸ªå¤´å¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›ï¼Œæå‡æ¨¡å‹å¯¹ä¸åŒç‰¹å¾çš„æ•æ‰èƒ½åŠ›ï¼ˆéœ€æ»¡è¶³ hidden_size % num_attention_heads == 0ï¼Œå¦åˆ™æ— æ³•å‡åˆ†ç»´åº¦ï¼‰ã€‚
            num_attention_heads: int = 8,
            # 	Transformer éšè—å±‚æ•°é‡ï¼ˆå³ç¼–ç å™¨ / è§£ç å™¨çš„å±‚æ•°ï¼‰ï¼Œå±‚æ•°è¶Šå¤šæ¨¡å‹æ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºï¼Œä½†è®­ç»ƒæˆæœ¬è¶Šé«˜ã€‚
            num_hidden_layers: int = 8,
            # KV å¤´æ•°ï¼ˆç”¨äºåˆ†ç»„æ³¨æ„åŠ› / FlashAttentionï¼‰ï¼šåœ¨é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå°† Key/Value æŠ•å½±åˆ° fewer ä¸ªå¤´ä¸Šï¼ˆå¦‚ 2 ä¸ªï¼‰ï¼Œå‡å°‘è®¡ç®—é‡ï¼ˆéœ€æ»¡è¶³ num_attention_heads % num_key_value_heads == 0ï¼‰ã€‚
            num_key_value_heads: int = 2,
            # è¯è¡¨å¤§å°ï¼šæ¨¡å‹æ”¯æŒçš„å”¯ä¸€ token æ•°é‡ï¼ˆåŒ…æ‹¬å­—ç¬¦ã€å­è¯ç­‰ï¼‰ï¼Œéœ€ä¸è¯è¡¨æ–‡ä»¶ï¼ˆå¦‚ vocab.jsonï¼‰çš„å¤§å°ä¸€è‡´ã€‚
            vocab_size: int = 6400,
            # RMSNorm å½’ä¸€åŒ–çš„æå°å€¼ï¼šç”¨äºé¿å…åˆ†æ¯ä¸º 0ï¼ŒRMSNorm æ˜¯å¤§æ¨¡å‹å¸¸ç”¨çš„å½’ä¸€åŒ–æ–¹å¼ï¼ˆæ¯” LayerNorm è®¡ç®—æ›´é«˜æ•ˆï¼‰ã€‚
            rms_norm_eps: float = 1e-05,
            # RoPE ä½ç½®ç¼–ç çš„ theta å‚æ•°ï¼šRoPEï¼ˆRotary Position Embeddingï¼‰é€šè¿‡æ—‹è½¬çŸ©é˜µæ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œtheta å†³å®šä½ç½®ç¼–ç çš„å‘¨æœŸï¼ˆå€¼è¶Šå¤§ï¼Œå‘¨æœŸè¶Šé•¿ï¼Œé€‚åˆé•¿æ–‡æœ¬ï¼‰ã€‚
            rope_theta: int = 1000000.0,
            # æ˜¯å¦å¯ç”¨ RoPE é•¿åº¦å¤–æ¨ï¼šæ¨ç†æ—¶è‹¥è¾“å…¥æ–‡æœ¬é•¿åº¦è¶…è¿‡ max_position_embeddingsï¼Œé€šè¿‡ç¼©æ”¾ RoPE å‚æ•°é¿å…ä½ç½®ç¼–ç å¤±æ•ˆï¼ˆå¦‚ YARN æ–¹æ³•ï¼‰ã€‚
            inference_rope_scaling: bool = False,
            # æ˜¯å¦å¯ç”¨ FlashAttentionï¼šFacebook æå‡ºçš„é«˜æ•ˆæ³¨æ„åŠ›å®ç°ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨å’Œè®¡ç®—æ—¶é—´ï¼Œæ˜¯å¤§æ¨¡å‹è®­ç»ƒ / æ¨ç†çš„å¸¸ç”¨ä¼˜åŒ–ã€‚
            flash_attn: bool = True,
            ####################################################
            # Here are the specific configurations of MOE
            # When use_moe is false, the following is invalid
            ####################################################
            # æ˜¯å¦å¯ç”¨ MOE ç»“æ„ï¼šTrue è¡¨ç¤ºæ¨¡å‹ä½¿ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼ŒFalse åˆ™ä¸ºæ™®é€š Transformerã€‚
            
            # ä¸å°±æ˜¯å¤šä¸ªå…¨è¿æ¥å±‚ï¼ˆä¸åŒå…¨è¿æ¥å±‚ä»£è¡¨æ”¾å¤§ä¸åŒæ–¹é¢çš„ç‰¹å¾ï¼‰ï¼Œ
            # ç”¨ä¸€ä¸ªç½‘ç»œï¼ˆè·¯ç”±å™¨ï¼‰æ ¹æ®sigmiodï¼Œå†³å®šå“ªå‡ ä¸ªå…¨è¿æ¥å±‚æ¥æ”¶è¾“å…¥ï¼Œå¹¶å’Œé€šç”¨çš„å…¨è¿æ¥å±‚åŠ æƒæ±‚å’Œ
            use_moe: bool = False,
            # æ¯ä¸ª token é€‰æ‹©çš„ä¸“å®¶æ•°ï¼šMOE ä¸­æ¯ä¸ª token ä»…ç”± top-k ä¸ªä¸“å®¶å¤„ç†ï¼ˆå¦‚ 2 ä¸ªï¼‰ï¼Œå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—é‡ã€‚
            num_experts_per_tok: int = 2,
            # å¯è·¯ç”±ä¸“å®¶æ€»æ•°ï¼šæ¨¡å‹ä¸­ç‹¬ç«‹çš„ä¸“å®¶ç½‘ç»œæ•°é‡ï¼ˆå¦‚ 4 ä¸ªï¼‰ï¼Œæ¯ä¸ªä¸“å®¶è´Ÿè´£å¤„ç†ç‰¹å®šç±»å‹çš„ tokenã€‚
            n_routed_experts: int = 4,
            # å…±äº«ä¸“å®¶æ•°é‡ï¼šæ‰€æœ‰ token éƒ½ä¼šç»è¿‡çš„ã€Œå…±äº«ä¸“å®¶ã€ï¼ˆåŒºåˆ«äºã€Œå¯è·¯ç”±ä¸“å®¶ã€ï¼‰ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼ˆé¿å…éƒ¨åˆ†ä¸“å®¶è¢«é—²ç½®ï¼‰ã€‚
            n_shared_experts: int = 1,
            # ä¸“å®¶é€‰æ‹©çš„è¯„åˆ†å‡½æ•°ï¼šè®¡ç®—æ¯ä¸ª token ä¸ä¸“å®¶çš„åŒ¹é…åº¦ï¼Œsoftmax ä¼šå°†è¯„åˆ†å½’ä¸€åŒ–ä¸ºæ¦‚ç‡ï¼Œå…¶ä»–å¯é€‰å¦‚ sigmoidã€‚
            scoring_func: str = 'softmax',
            # è¾…åŠ©æŸå¤±çš„æƒé‡ï¼šMOE ä¸­ä¸ºé¿å…ã€Œä¸“å®¶é—²ç½®ã€ï¼ˆéƒ¨åˆ†ä¸“å®¶å‡ ä¹ä¸è¢«é€‰æ‹©ï¼‰ï¼Œæ·»åŠ è¾…åŠ©æŸå¤±ï¼ˆå¦‚ä¸“å®¶å‡è¡¡æŸå¤±ï¼‰ï¼Œalpha æ§åˆ¶è¾…åŠ©æŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„å æ¯”ã€‚
            aux_loss_alpha: float = 0.1,
            # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤±ï¼šTrue è¡¨ç¤ºåŸºäºæ•´ä¸ªåºåˆ—çš„ä¸“å®¶é€‰æ‹©æƒ…å†µè®¡ç®—å‡è¡¡æŸå¤±ï¼ŒFalse åˆ™æŒ‰å•ä¸ª token è®¡ç®—ã€‚
            seq_aux: bool = True,
            # æ˜¯å¦æ ‡å‡†åŒ– top-k ä¸“å®¶çš„æ¦‚ç‡ï¼šTrue ä¼šå°†é€‰ä¸­çš„ k ä¸ªä¸“å®¶çš„æ¦‚ç‡é‡æ–°å½’ä¸€åŒ–ï¼Œç¡®ä¿æ¦‚ç‡å’Œä¸º 1ï¼Œæå‡ç¨³å®šæ€§ã€‚
            norm_topk_prob: bool = True,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        # å¤–æ¨é•¿åº¦ = factor * original_max_position_embeddings
        # ä¿®æ”¹æ¨¡å‹å¯¹ â€œä½ç½®ä¿¡æ¯â€ çš„å¤„ç†æ–¹å¼ï¼Œè®©æ¨¡å‹è¯¯ä»¥ä¸º â€œè¶…é•¿æ–‡æœ¬çš„ä½ç½®â€ ä»åœ¨è‡ªå·±ç†Ÿæ‚‰çš„ â€œè®­ç»ƒçª—å£â€ å†…ï¼Œä»è€Œæ­£å¸¸ç†è§£å†…å®¹

        self.rope_scaling = {
            "beta_fast": 4,
            "beta_slow": 1,
            "factor": 4,
            "original_max_position_embeddings": 2048,
            "type": "yarn"
        } if self.inference_rope_scaling else None
        self.flash_attn = flash_attn
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

# Python å†…ç½®çš„æ•°å­¦å·¥å…·åº“ï¼Œæä¾›åŸºç¡€æ•°å­¦è¿ç®—æ”¯æŒ
import math
# PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶çš„æ ¸å¿ƒåº“ï¼Œæ˜¯æ‰€æœ‰æ¨¡å‹è®­ç»ƒ / æ¨ç†çš„åŸºç¡€,æä¾›å¤šç§å‚æ•°åˆå§‹åŒ–æ–¹æ³•ï¼Œé¿å…æ¨¡å‹è®­ç»ƒæ—¶å› å‚æ•°åˆå§‹å€¼ä¸å½“å¯¼è‡´çš„æ¢¯åº¦æ¶ˆå¤± / çˆ†ç‚¸
# æä¾›å¼ é‡ï¼ˆTensorï¼‰æ“ä½œã€è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰ã€GPU åŠ é€Ÿã€ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆnnï¼‰ç­‰æ ¸å¿ƒèƒ½åŠ›ï¼Œåç»­æ‰€æœ‰æ¨¡å‹çš„å‚æ•°ï¼ˆæƒé‡ï¼‰ã€è¾“å…¥æ•°æ®éƒ½ä»¥ PyTorch å¼ é‡å½¢å¼å­˜å‚¨å’Œå¤„ç†ã€‚
import torch
# PyTorch ä¸­ç¥ç»ç½‘ç»œå‚æ•°çš„åˆå§‹åŒ–å·¥å…·æ¨¡å—ï¼Œç¼©å†™ä¸º initï¼ˆçº¦å®šä¿—æˆçš„ç®€å†™ï¼Œæ–¹ä¾¿è°ƒç”¨ï¼‰
import torch.nn.init as init
# æä¾›æ— çŠ¶æ€çš„ç¥ç»ç½‘ç»œæ“ä½œï¼ˆå³æ“ä½œæœ¬èº«ä¸å­˜å‚¨å‚æ•°ï¼Œä»…æ¥æ”¶è¾“å…¥å’Œå‚æ•°è®¡ç®—è¾“å‡ºï¼‰
# è°ƒç”¨æ¿€æ´»å‡½æ•°ï¼ˆF.siluã€F.geluï¼‰ã€æŸå¤±å‡½æ•°ï¼ˆF.cross_entropyã€F.mse_lossï¼‰
import torch.nn.functional as F
# æä¾›å°è£…å¥½çš„å¯è®­ç»ƒæ¨¡å—ï¼ˆç±»ï¼‰ï¼Œè¿™äº›æ¨¡å—ä¼šè‡ªåŠ¨ç®¡ç†å†…éƒ¨å‚æ•°ï¼ˆæƒé‡ã€åç½®ï¼‰ï¼Œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼å’Œå‚æ•°ä¼˜åŒ–ã€‚
# å®šä¹‰æ¨¡å‹å±‚ç»“æ„ï¼Œä¾‹å¦‚å…¨è¿æ¥å±‚ï¼ˆnn.Linearï¼‰ã€å½’ä¸€åŒ–å±‚ï¼ˆnn.RMSNormï¼‰ã€Dropout å±‚ï¼ˆnn.Dropoutï¼‰ã€Embedding å±‚ï¼ˆnn.Embeddingï¼‰ç­‰ï¼Œåç»­æ„å»º Transformer/MOE æ¨¡å‹çš„å±‚éƒ½ä¼šä¾èµ– nn æ¨¡å—ã€‚
from torch import nn
# å½“é…ç½®ä¸­æŒ‡å®š hidden_act='silu' æ—¶ï¼Œå¯é€šè¿‡ ACT2FN[config.hidden_act] ç›´æ¥è·å–å¯¹åº”çš„æ¿€æ´»å‡½æ•°ï¼ˆæ— éœ€æ‰‹åŠ¨åˆ¤æ–­å­—ç¬¦ä¸²å¯¹åº”çš„å‡½æ•°ï¼‰
from transformers.activations import ACT2FN
from typing import Optional, Tuple, List, Union
# ä» transformers åº“å¯¼å…¥æ„å»ºè‡ªå®šä¹‰æ¨¡å‹çš„æ ¸å¿ƒåŸºç±»ï¼Œæ˜¯å…¼å®¹ transformers ç”Ÿæ€çš„å…³é”®ã€‚
# PreTrainedModel: æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„çˆ¶ç±»ï¼Œæä¾›æ¨¡å‹åŠ è½½ / ä¿å­˜ï¼ˆfrom_pretrained/save_pretrainedï¼‰ã€è®¾å¤‡è¿ç§»ï¼ˆto(device)ï¼‰ã€å‚æ•°å†»ç»“ç­‰é€šç”¨åŠŸèƒ½ï¼Œè‡ªå®šä¹‰æ¨¡å‹ï¼ˆå¦‚ MiniMindModelï¼‰éœ€ç»§æ‰¿æ­¤ç±»ã€‚
# GenerationMixinï¼š ç”Ÿæˆå¼æ¨¡å‹çš„æ··å…¥ç±»ï¼ˆMixinï¼‰ï¼Œæä¾›æ–‡æœ¬ç”Ÿæˆçš„æ ¸å¿ƒæ–¹æ³•ï¼ˆå¦‚ generate()ï¼‰ï¼ŒåŒ…å«è´ªå¿ƒæœç´¢ã€æŸæœç´¢ï¼ˆBeam Searchï¼‰ç­‰ç”Ÿæˆç­–ç•¥ï¼Œè®©è‡ªå®šä¹‰æ¨¡å‹æ— éœ€æ‰‹åŠ¨å®ç°ç”Ÿæˆé€»è¾‘ã€‚
# ä¹‹å‰è®²è§£è¿‡çš„é…ç½®ç±»çˆ¶ç±»ï¼Œæ­¤å¤„å¯¼å…¥æ˜¯ä¸ºäº†åœ¨æ¨¡å‹ç±»ä¸­æ¥æ”¶é…ç½®å®ä¾‹ï¼ˆå¦‚ __init__(self, config: PretrainedConfig)ï¼‰ï¼Œç¡®ä¿æ¨¡å‹ä¸é…ç½®çš„è”åŠ¨ã€‚
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
'''
ç»Ÿä¸€ç”Ÿæˆå¼æ¨¡å‹çš„è¾“å‡ºæ ¼å¼ï¼Œå°†æ¨¡å‹çš„æ ¸å¿ƒè¾“å‡ºï¼ˆé¢„æµ‹ logitsã€éšè—çŠ¶æ€ã€æ³¨æ„åŠ›æƒé‡ç­‰ï¼‰å°è£…æˆä¸€ä¸ªå…·åå…ƒç»„ï¼ˆNamed Tupleï¼‰ï¼Œæ–¹ä¾¿åç»­è°ƒç”¨ï¼ˆå¦‚è®¡ç®—æŸå¤±ã€ç”Ÿæˆæ–‡æœ¬æ—¶è·å–ä¸­é—´ç»“æœï¼‰ã€‚
è¾“å‡ºç±»åŒ…å«çš„å…³é”®å±æ€§ï¼ˆæŒ‰éœ€ä½¿ç”¨ï¼‰ï¼š
logits: æ¨¡å‹æœ€ç»ˆé¢„æµ‹çš„ token æ¦‚ç‡åˆ†å¸ƒï¼ˆshape: [batch_size, seq_len, vocab_size]ï¼‰ï¼Œç”¨äºè®¡ç®—æŸå¤±æˆ–é‡‡æ ·ä¸‹ä¸€ä¸ª tokenï¼›
past_key_values: ç¼“å­˜çš„æ³¨æ„åŠ›å±‚ Key/Value å¼ é‡ï¼Œç”¨äºå¢é‡ç”Ÿæˆï¼ˆé¿å…é‡å¤è®¡ç®—å·²ç”Ÿæˆ token çš„æ³¨æ„åŠ›ï¼Œæå‡ç”Ÿæˆé€Ÿåº¦ï¼‰ï¼›
hidden_states: æ¨¡å‹å„å±‚çš„éšè—çŠ¶æ€ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºç‰¹å¾æå–æˆ–è°ƒè¯•ï¼›
attentions: å„æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºå¯è§†åŒ–æˆ–åˆ†ææ¨¡å‹æ³¨æ„åŠ›åˆ†å¸ƒã€‚
'''
from transformers.modeling_outputs import CausalLMOutputWithPast

'''
ä¸ºä»€ä¹ˆç»§æ‰¿ nn.Moduleï¼š
è·å¾— PyTorch å†…ç½®çš„å‚æ•°ç®¡ç†ï¼ˆå¦‚ nn.Parameter è‡ªåŠ¨æ³¨å†Œä¸ºå¯è®­ç»ƒå‚æ•°ï¼‰ã€è®¾å¤‡è¿ç§»ï¼ˆto(device)ï¼‰ã€å‰å‘ä¼ æ’­æ¥å£ï¼ˆforward æ–¹æ³•ï¼‰ç­‰æ ¸å¿ƒåŠŸèƒ½ï¼›
ç¡®ä¿è¯¥å±‚èƒ½åƒ nn.Linearã€nn.Dropout ä¸€æ ·ï¼ŒåµŒå…¥åˆ°å®Œæ•´çš„ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨ã€‚

é€šè¿‡ â€œæ ‡å‡†åŒ– + è‡ªé€‚åº”ç¼©æ”¾â€ï¼Œè®©æ¨¡å‹ä¸­æ¯ä¸ª token çš„éšè—å‘é‡å¹…åº¦ä¿æŒä¸€è‡´ï¼Œé¿å…å› æ•°å€¼è¿‡å¤§ / è¿‡å°å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šï¼ˆå¦‚æ¢¯åº¦æ¶ˆå¤± / çˆ†ç‚¸ï¼‰ã€‚
ç›¸æ¯”ä¼ ç»Ÿ LayerNormï¼ŒRMSNorm å°‘äº†ã€Œå‡å‡å€¼ã€çš„æ­¥éª¤ï¼Œè®¡ç®—é‡æ›´å°ã€æ˜¾å­˜å ç”¨æ›´ä½ï¼Œæ˜¯å¤§æ¨¡å‹ï¼ˆå¦‚ LLaMAã€GPT-4ï¼‰çš„ä¸»æµé€‰æ‹©ã€‚
åœ¨ Transformer å±‚ï¼ˆæˆ– MOE ä¸“å®¶å±‚ï¼‰çš„è¾“å…¥ / è¾“å‡ºå¤„æ’å…¥è¯¥å±‚ï¼Œä¼ å…¥ eps=config.rms_norm_eps å³å¯
'''
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    # å®šä¹‰æ ¸å¿ƒçš„å½’ä¸€åŒ–è®¡ç®—é€»è¾‘ï¼Œç”¨ä¸‹åˆ’çº¿ _ å¼€å¤´è¡¨ç¤ºã€Œå†…éƒ¨è¾…åŠ©å‡½æ•°ã€ï¼ˆä¸å»ºè®®å¤–éƒ¨ç›´æ¥è°ƒç”¨ï¼Œä»…åœ¨ forward ä¸­ä½¿ç”¨ï¼‰ã€‚
    '''
    torch.rsqrt: è®¡ç®—å¹³æ–¹æ ¹çš„å€’æ•°
    x.pow(2): è®¡ç®—è¾“å…¥å¼ é‡æ¯ä¸ªä½ç½®çš„å¹³æ–¹ï¼ˆå¦‚ x=3 å˜æˆ 9ï¼Œx=-2 å˜æˆ 4ï¼‰ï¼Œç›®çš„æ˜¯æ¶ˆé™¤æ­£è´Ÿå·å½±å“ï¼Œèšç„¦æ•°å€¼å¤§å°
    x.pow(2).mean(-1, keepdim=True) :-1 è¡¨ç¤ºã€Œæœ€åä¸€ä¸ªç»´åº¦ã€ï¼ˆå³ dim ç»´åº¦ï¼‰ï¼Œkeepdim=True è¡¨ç¤ºä¿æŒç»´åº¦ä¸å˜ï¼ˆè¾“å…¥ [32,128,512] â†’ è¾“å‡º [32,128,1]ï¼‰ï¼Œé¿å…å¹¿æ’­è®¡ç®—å‡ºé”™ï¼›ç»“æœæ˜¯æ¯ä¸ª token 512 ç»´å‘é‡çš„ã€Œå¹³æ–¹å‡å€¼ã€ï¼ˆè¡¡é‡è¯¥ token å‘é‡çš„æ•´ä½“å¹…åº¦ï¼‰
    x * ... ç”¨è¾“å…¥å¼ é‡ x ä¹˜ä»¥å½’ä¸€åŒ–ç³»æ•°ï¼Œæœ€ç»ˆå¾—åˆ°ã€Œå‡å€¼ä¸º 0ã€æ–¹å·®è¿‘ä¼¼ä¸º 1ã€çš„æ ‡å‡†åŒ–å‘é‡ï¼ˆæ¶ˆé™¤ä¸åŒ token å‘é‡å¹…åº¦å·®å¼‚çš„å½±å“ï¼‰
    '''
    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    # å®šä¹‰æ•°æ®çš„å‰å‘æµåŠ¨é€»è¾‘
    # .type_as(x) é¿å…ç±»å‹ä¸åŒ¹é…ï¼ˆå¦‚è¾“å…¥æ˜¯ float16ï¼Œæ ‡å‡†åŒ–åä»è½¬å› float16ï¼Œä¿è¯åç»­è®¡ç®—å…¼å®¹ï¼‰
    # self.weight * ... ç”¨å¯è®­ç»ƒçš„ weightï¼ˆshape [dim]ï¼‰å¯¹æ ‡å‡†åŒ–å‘é‡çš„æ¯ä¸ªç»´åº¦é€å…ƒç´ ç›¸ä¹˜ï¼ˆå¹¿æ’­æœºåˆ¶ï¼‰ï¼Œå®ç° â€œè‡ªé€‚åº”ç¼©æ”¾â€
    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)

'''
Yarn é•¿åº¦å¤–æ¨

ç¬¬ä¸€æ­¥ï¼šå‘¨æœŸæ˜ å°„å…¬å¼
p' = p mod L + s * (L/K)   s=floor(p/L)

p mod L :ä¿è¯å§‹ç»ˆåœ¨ä¸€ä¸ªå‘¨æœŸå†…
s * (L/K): åŒºåˆ†ä¸åŒå‘¨æœŸä¸­ç›¸åŒä½ç½®
----------------------
æ‹†è§£ 1ï¼šp mod Lï¼ˆå–ä½™è¿ç®—ï¼‰â†’ å¯¹åº” â€œå‘¨æœŸå†…çš„åŸºç¡€ä½ç½®â€ã€‚æ¯”å¦‚ L=2000ï¼Œp=10000 æ—¶ï¼Œ10000 mod 2000 = 0ï¼Œå³ç¬¬ 10000 ä¸ª token å¯¹åº” â€œç¬¬ 5 ä¸ªå‘¨æœŸçš„ç¬¬ 0 ä¸ªä½ç½®â€ï¼ˆç±»ä¼¼ â€œç¬¬ 5 è½®çš„ç¬¬ 1 æœ¬ä¹¦â€ï¼‰ï¼›
âœ… ç”Ÿæ´»å¯¹åº”ï¼šç®¡ç†å‘˜ä¸ç”¨è®° â€œç¬¬ 10000 æœ¬â€ï¼Œåªè®° â€œè¿™æ˜¯å½“å‰å‘¨æœŸçš„ç¬¬ 0 æœ¬â€ï¼Œå’Œç¬¬ 1 ä¸ªå‘¨æœŸçš„ â€œç¬¬ 0 æœ¬â€ï¼ˆå³ç¬¬ 1 æœ¬ä¹¦ï¼‰ç¼–ç é€»è¾‘ä¸€è‡´ï¼Œä¸ä¼šæ‡µã€‚
æ‹†è§£ 2ï¼šs = floor(p / L)ï¼ˆå‘ä¸‹å–æ•´ï¼‰â†’ å¯¹åº” â€œå‘¨æœŸåºå·â€ã€‚p=10000 æ—¶ï¼Œs=10000/2000=5ï¼Œå³ç¬¬ 5 ä¸ªå‘¨æœŸï¼›
æ‹†è§£ 3ï¼šK æ˜¯ â€œå‘¨æœŸåˆ†ç»„æ•°â€ï¼ˆYARN é¢„è®¾çš„è¶…å‚æ•°ï¼Œæ¯”å¦‚ 4ï¼‰â†’ ç»™ä¸åŒå‘¨æœŸåŠ  â€œè½»å¾®åŒºåˆ†â€ï¼Œé¿å…æ¨¡å‹æ··æ·† â€œç¬¬ 1 å‘¨æœŸçš„ç¬¬ 0 æœ¬â€ å’Œ â€œç¬¬ 5 å‘¨æœŸçš„ç¬¬ 0 æœ¬â€ï¼Œä½†åŒºåˆ†åº¦å¾ˆå°ï¼Œä¸å½±å“æ¨¡å‹å¯¹ â€œå‘¨æœŸå†…é¡ºåºâ€ çš„è¯†åˆ«ã€‚
ï¼ˆ2ï¼‰ç›¸å¯¹ä½ç½®è¡¥å……ï¼ˆå…¬å¼éšå«é€»è¾‘ï¼‰
ä¼ ç»Ÿç¼–ç åªçœ‹ pï¼ˆç»å¯¹ä½ç½®ï¼‰ï¼ŒYARN é€šè¿‡å‘¨æœŸæ˜ å°„åï¼Œä¸¤ä¸ª token çš„ã€Œç›¸å¯¹ä½ç½®ã€å¯ä»¥é€šè¿‡ |p1' - p2'| è®¡ç®—ï¼ˆæ¯”å¦‚ç¬¬ 10000 ä¸ª token çš„ p1'=0ï¼Œç¬¬ 10001 ä¸ª token çš„ p2'=1ï¼Œç›¸å¯¹ä½ç½®æ˜¯ 1ï¼Œå³ â€œç›¸é‚»â€ï¼‰ã€‚âœ… ç”Ÿæ´»å¯¹åº”ï¼šç®¡ç†å‘˜é€šè¿‡ p' çš„å·®å€¼ï¼Œç›´æ¥çŸ¥é“ â€œä¸¤æœ¬ä¹¦çš„å‰åå…³ç³»â€ï¼Œä¸ç”¨ç®¡å®ƒä»¬åœ¨å“ªä¸ªå‘¨æœŸã€‚

ç¬¬äºŒæ­¥ï¼šåŠ¨æ€ä¸Šä¸‹æ–‡å‹ç¼©
ï¼ˆ1ï¼‰å‹ç¼©å…¬å¼
æŠŠLä¸ªtokenï¼Œåˆ†æˆMç»„ï¼Œæ¯ç»„å¾—åˆ°æ¯ä¸ªtokençš„é‡è¦æ€§ï¼ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªä»£è¡¨æ€§çš„ä»£è¡¨æ€§ï¼ˆå…±Mä¸ªï¼‰
L - M = Nï¼Œ ä¼šåœ¨å¢åŠ Nä¸ª
--------------------
YARN ä¼šé€šè¿‡ã€Œæ³¨æ„åŠ›æ± åŒ–ï¼ˆAttention Poolingï¼‰ã€å¯¹æ—©æœŸä¸Šä¸‹æ–‡è¿›è¡Œå‹ç¼©ï¼Œå¾—åˆ°Â MÂ ä¸ªæ ¸å¿ƒè¯­ä¹‰å‘é‡Â H_compressedï¼š\(H_{compressed} = \text{AttentionPool}(H, W) = \sum_{i=1}^L \alpha_i \cdot h_i\)æ‹†è§£ 1ï¼šÎ±_i = softmax(W Â· h_i)ï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰â†’Â Î±_iÂ æ˜¯ç¬¬Â iÂ ä¸ª token çš„ â€œé‡è¦æ€§å¾—åˆ†â€ï¼Œæ€»å’Œä¸º 1ï¼ˆæ¯”å¦‚é‡è¦çš„ token å¾—åˆ†Â Î±_i=0.01ï¼Œå†—ä½™çš„ token å¾—åˆ†Â Î±_i=0.0001ï¼‰ï¼›
âœ… ç”Ÿæ´»å¯¹åº”ï¼šç®¡ç†å‘˜åˆ¤æ–­ â€œç¬¬ 3 æœ¬ä¹¦è®²æ ¸å¿ƒåŸç†ï¼ˆÎ±_i é«˜ï¼‰ï¼Œç¬¬ 5 æœ¬ä¹¦æ˜¯é‡å¤ä¸¾ä¾‹ï¼ˆÎ±_i ä½ï¼‰â€ï¼Œé‡ç‚¹è®°å‰è€…ã€‚

æ‹†è§£ 2ï¼šsum(Î±_i Â· h_i)Â â†’ åŠ æƒæ±‚å’Œï¼ŒæŠŠÂ LÂ ä¸ªè¯­ä¹‰å‘é‡ â€œæµ“ç¼©â€ æˆÂ MÂ ä¸ªï¼ˆYARN ä¼šåˆ†Â MÂ ç»„è®¡ç®—ï¼Œæ¯ç»„å¯¹åº”ä¸€ä¸ªæ ¸å¿ƒå‘é‡ï¼‰ï¼Œç›¸å½“äº â€œæŠŠ 1000 æœ¬ä¹¦çš„é‡ç‚¹ï¼Œæç‚¼æˆ 1 æ¡ç¬”è®°â€ï¼›
âœ… ç”Ÿæ´»å¯¹åº”ï¼šç®¡ç†å‘˜ä¸è®°æ¯æœ¬ä¹¦çš„é€å­—å†…å®¹ï¼Œåªè®° â€œè¿™ 1000 æœ¬ä¹¦çš„æ ¸å¿ƒæ˜¯ XXâ€ï¼Œç¬”è®°ä½“ç§¯å°ï¼ˆå ç©ºé—´å°‘ï¼‰ä½†ä¿¡æ¯å¯†åº¦é«˜ã€‚

ï¼ˆ2ï¼‰çª—å£æ›´æ–°é€»è¾‘ï¼ˆå…¬å¼éšå«æµç¨‹ï¼‰å½“æ–°çš„Â NÂ ä¸ª token è¿›æ¥ï¼ˆæ¯”å¦‚æ–°çš„ 1000 æœ¬ä¹¦ï¼‰ï¼Œçª—å£æ›´æ–°å…¬å¼ä¸ºï¼š\(H_{new} = [H_{compressed}, h_{L+1}, h_{L+2}, ..., h_{L+N}]\)æ‹†è§£ï¼šæŠŠå‹ç¼©åçš„Â MÂ ä¸ªæ ¸å¿ƒå‘é‡ï¼ˆç¬”è®°æœ¬ç¬”è®°ï¼‰ï¼Œå’Œæ–°çš„Â NÂ ä¸ª token è¯­ä¹‰å‘é‡ï¼ˆæ–°æ‘†ä¸Šæ¡Œé¢çš„ä¹¦ï¼‰æ‹¼æ¥ï¼Œæ€»é•¿åº¦ä»ä¸ºÂ Lï¼ˆM + N = Lï¼‰ï¼Œæ—¢æ²¡è¶…å‡ºçª—å£ï¼Œåˆä¿ç•™äº†å‰æ–‡é‡ç‚¹ï¼›
âœ… ç”Ÿæ´»å¯¹åº”ï¼šæ¡Œé¢å§‹ç»ˆä¿æŒ 2000 ä¸ª â€œä¿¡æ¯å•å…ƒâ€ï¼ˆ1000 æ¡ç¬”è®° + 1000 æœ¬æ–°ä¹¦ï¼‰ï¼Œä¸ä¼šæº¢å‡ºï¼Œä¸”ç¬”è®°èƒ½æ›¿ä»£åŸä¹¦çš„æ ¸å¿ƒä¿¡æ¯ã€‚


1.
åœ¨ Word2Vec / Transformer è¯­ä¹‰ç©ºé—´ä¸­ï¼Œ
â€œè¯­ä¹‰â€å…¶å®ä¸æ˜¯æŒ‡æŸä¸ªå…·ä½“åæ ‡å€¼ï¼Œè€Œæ˜¯æŒ‡ï¼š
åœ¨è¿™ä¸ªç©ºé—´é‡Œï¼Œä¸¤ä¸ªè¯å‘é‡ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚
æ¯”å¦‚ï¼š king - man + woman â‰ˆ queen
è¿™åªä¾èµ–äºæ–¹å‘å’Œè§’åº¦ï¼Œä¸ä¾èµ–äºç»å¯¹åæ ‡ã€‚
æ‰€ä»¥ RoPE åšçš„äº‹æƒ…å°±åƒï¼š
â€œç»™æ•´ä¸ªè¯­ä¹‰ç©ºé—´åŠ äº†ä¸€ä¸ªä½ç½®ç›¸å…³çš„æ—‹è½¬æ»¤é•œï¼Œ
è®©æ¨¡å‹èƒ½åŒºåˆ†é¡ºåºä¿¡æ¯ï¼Œ
ä½†ä¸ç ´åè¯­ä¹‰çš„å‡ ä½•ç»“æ„ã€‚â€

2.
RoPE çš„ä½œç”¨æ˜¯ï¼š
â€œç»™æ•´ä¸ªè¯­ä¹‰ç©ºé—´åŠ äº†ä¸€ä¸ªä½ç½®ç›¸å…³çš„æ—‹è½¬æ»¤é•œï¼Œ
å³æ·»åŠ ä¸€ä¸ªæ­£äº¤çš„çŸ©é˜µ,ä½¿å¾—æ•´ä¸ªç©ºé—´éšä¹‹å˜åŠ¨,ç”±äºæ­£äº¤æ€§,æ—‹è½¬åçš„å‘é‡è§’åº¦ç›¸å¯¹ä¸å˜,æ¨¡é•¿ä¸å˜,å¹¶æ²¡æœ‰æ”¹å˜è¯­ä¹‰.
è®©æ¨¡å‹èƒ½åŒºåˆ†é¡ºåºä¿¡æ¯ï¼Œ
ä½†ä¸ç ´åè¯­ä¹‰çš„å‡ ä½•ç»“æ„ã€‚â€

3.


Yarn-->RoPE->embedding-->word2verc
ä¸ºäº†è®©æ¨¡å‹å¤„ç†æ–‡æœ¬ï¼Œéœ€è¦å°†æ–‡æœ¬æ•°å€¼åŒ–ï¼Œé™¤äº†one-hotè¿™ç§æ–¹å¼å¤–ï¼Œä½†è¿™ä¼šæœ‰å”¯ç‹¬ç¾éš¾ï¼ˆæœ‰å¤§é‡çš„æ— æ•ˆçš„0ï¼Œä¸”ä¸èƒ½è¡¨ç¤ºè¯­ä¹‰ï¼‰ï¼Œä½†å¦‚æœç”¨ä¸€ç§ç¨ å¯†å‘é‡è¡¨ç¤ºå•ä¸ªè¯è¯­(token)ï¼Œè®©ç›¸ä¼¼è¯­ä¹‰çš„è¯è¯­(token)å‘é‡å¤¹è§’è¾ƒå°ï¼Œç›¸åŒè¯­ä¹‰ï¼ˆè¯­ä¹‰è¾ƒå¼ºçš„ï¼Œæ¨¡é•¿çš„å¤§ï¼Œè¯­ä¹‰å°çš„ï¼Œæ¨¡é•¿å°ï¼‰ï¼Œ
ä½†æ€ä¹ˆåšåˆ°å‘¢ï¼Ÿæ–¹æ³•å°±æ˜¯ä½¿ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œè®­ç»ƒæ¨¡å‹ï¼Œå¼ºè¿«æ¨¡å‹å°†è¯­ä¹‰ç›¸ä¼¼çš„ï¼Œå‘é‡å¤¹è§’ç›¸è¿‘ã€‚æ¨¡å‹ä¸ä¼šåˆ»æ„æ§åˆ¶æ¯ä¸ªç»´åº¦çš„æ¯”ä¾‹å…³ç³»ï¼Œè€Œæ˜¯è®©æ•´ä½“çš„æ–¹å‘å…³ç³»é€šè¿‡ä¼˜åŒ–å†…ç§¯æ¥è‡ªç„¶å½¢æˆã€‚è¯­ä¹‰ç”±å‘é‡é—´çš„ç›¸å¯¹æ–¹å‘ï¼ˆå•ä½å‘é‡ / å¤¹è§’ / cosineï¼‰ç¡®å®šï¼›æ¨¡é•¿æ˜¯å¼ºåº¦
æ‰€æœ‰å‘é‡ä¹‹é—´çš„ç›¸å¯¹è§’åº¦ä¸å˜ã€‚è¯­ä¹‰ä¾èµ–çš„æ˜¯è¿™ç§å…¨å±€ç›¸å¯¹è§’åº¦ç»“æ„ï¼Œå› æ­¤è¯­ä¹‰ä¿æŒã€‚
å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š
æˆ‘ä»¬ç”¨ SimCSE è®­ç»ƒå¥å‘é‡ çš„åœºæ™¯åšå®é™…è®­ç»ƒä¾‹å­ â€”â€” è¿™æ˜¯å¯¹æ¯”å­¦ä¹ ï¼ˆInfoNCE Lossï¼‰æœ€ç»å…¸çš„åº”ç”¨ï¼Œå…¨ç¨‹è¿˜åŸ â€œæ•°æ®å‡†å¤‡â†’æ¨¡å‹è®¡ç®—â†’æŸå¤±ä¼˜åŒ–â†’å‘é‡æ”¶æ•›â€ çš„å®Œæ•´è¿‡ç¨‹ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½å¯¹åº”å…¬å¼ï¼Œç›´è§‚çœ‹åˆ°æŸå¤±å‡½æ•°å¦‚ä½• â€œé€¼ç€â€ å‘é‡æ»¡è¶³è¯­ä¹‰çº¦æŸã€‚
è®­ç»ƒå®Œæ¯•åï¼Œç›¸å½“äºç›¸åŒè¯­ä¹‰çš„ï¼Œå‘é‡åœ¨åŒä¸€é™„è¿‘ï¼Œå®é™…è·Ÿäººçš„æ€æƒ³æœ‰ç‚¹åƒï¼Œè°ˆä¸€ä¸ªè¯é¢˜ï¼Œç›¸åŒè¯­ä¹‰çš„å¤šä¸ªè¯ï¼Œç›¸ç»§å‡ºç°çš„æ¦‚ç‡å¤§ã€‚
æ¨¡å‹ä¸ç†è§£ç°å®çš„æ„æ€ï¼Œä»–åªæ˜¯æ˜ç™½æ ¹æ®å‰é¢çš„å¤šä¸ªå‘é‡ï¼Œå‘é‡çš„é¡ºåºï¼Œä¸‹ä¸€ä¸ªå‘é‡åº”è¯¥æ˜¯è¿™ä¸ªï¼Œç„¶åè¾“å‡ºï¼Œè½¬æ¢ä¸ºäººç†è§£çš„è¯è¯­ï¼Œçœ‹ç€å¤§æ¨¡å‹ä¼¼ä¹ç†è§£äº†ï¼Œæˆ‘çš„æ„æ€ï¼Œå®é™…ä¸Šä¸æ˜¯ï¼Œåªæ˜¯é‚£ä¸ªå‘é‡è¢«è®¡ç®—å‡ºæ˜¯ä¸‹ä¸€ä¸ªå‘é‡çš„æ¦‚ç‡å¤§ã€‚

è®­ç»ƒè¿‡ç¨‹ï¼š12



ä¸€ã€è®­ç»ƒä»»åŠ¡å®šä¹‰
ç›®æ ‡ï¼šè®­ç»ƒæ¨¡å‹è®©ã€ŒåŒä¹‰å¥å‘é‡å¤¹è§’æ¥è¿‘ 0Â°ï¼ŒéåŒä¹‰å¥å‘é‡å¤¹è§’æ¥è¿‘ 180Â°ã€ã€‚é€‰ç”¨ 3 ä¸ªå¥å­ä½œä¸ºè®­ç»ƒæ ·æœ¬ï¼ˆæ¨¡æ‹Ÿæµ·é‡è®­ç»ƒæ•°æ®ä¸­çš„ä¸€ä¸ªæ‰¹æ¬¡ï¼‰ï¼š
é”šç‚¹å¥ï¼ˆxï¼‰ï¼šâ€œæˆ‘çˆ±åƒè‹¹æœâ€ï¼ˆæ ¸å¿ƒè¯­ä¹‰ï¼šè‹¹æœç›¸å…³ï¼‰
æ­£æ ·æœ¬å¥ï¼ˆxâºï¼‰ï¼šâ€œæˆ‘å–œæ¬¢åƒè‹¹æœâ€ï¼ˆå’Œé”šç‚¹åŒä¹‰ï¼Œè¯­ä¹‰ç›¸ä¼¼ï¼‰
è´Ÿæ ·æœ¬å¥ï¼ˆxâ»â‚, xâ»â‚‚ï¼‰ï¼šâ€œæˆ‘çˆ±åƒæ±½è½¦â€â€œçŸ³å¤´å¾ˆç¡¬â€ï¼ˆå’Œé”šç‚¹ä¸åŒä¹‰ï¼Œè¯­ä¹‰ä¸ç›¸ä¼¼ï¼‰
æ¨¡å‹ï¼šç”¨ç®€åŒ–ç‰ˆ BERTï¼ˆä»…ä¿ç•™ç¼–ç å™¨å’Œå¥å‘é‡è¾“å‡ºå±‚ï¼‰ï¼Œå¥å‘é‡ç»´åº¦ä¸º 2ï¼ˆæ–¹ä¾¿è®¡ç®—å’Œå¯è§†åŒ–ï¼Œå®é™…æ˜¯ 768 ç»´ï¼Œé€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰ã€‚è¶…å‚æ•°ï¼šæ¸©åº¦ Ï„=0.1ï¼ˆæ§åˆ¶åŒºåˆ†åº¦ï¼Œä¸»æµå–å€¼ï¼‰ã€‚
äºŒã€Step1ï¼šæ•°æ®å‡†å¤‡ä¸å‘é‡åˆå§‹åŒ–
è®­ç»ƒåˆšå¼€å§‹æ—¶ï¼Œæ¨¡å‹çš„å¥å‘é‡æ˜¯ éšæœºåˆå§‹åŒ– çš„ï¼ˆå®Œå…¨ä¸ç¬¦åˆè¯­ä¹‰ï¼‰ï¼Œæˆ‘ä»¬å…ˆè®°å½•åˆå§‹å‘é‡ï¼ˆéšæœºç”Ÿæˆåˆç†èŒƒå›´çš„æ•°å€¼ï¼‰ï¼š
é”šç‚¹å‘é‡ x = [0.2, 0.3]áµ€ï¼ˆé•¿åº¦âˆ¥xâˆ¥=âˆš(0.2Â²+0.3Â²)â‰ˆ0.36ï¼‰
æ­£æ ·æœ¬å‘é‡ xâº = [0.5, 0.1]áµ€ï¼ˆé•¿åº¦âˆ¥xâºâˆ¥=âˆš(0.5Â²+0.1Â²)â‰ˆ0.51ï¼‰
è´Ÿæ ·æœ¬å‘é‡ xâ»â‚ = [0.7, 0.8]áµ€ï¼ˆé•¿åº¦âˆ¥xâ»â‚âˆ¥=âˆš(0.7Â²+0.8Â²)â‰ˆ1.06ï¼‰
è´Ÿæ ·æœ¬å‘é‡ xâ»â‚‚ = [0.1, 0.9]áµ€ï¼ˆé•¿åº¦âˆ¥xâ»â‚‚âˆ¥=âˆš(0.1Â²+0.9Â²)â‰ˆ0.91ï¼‰
æ­¤æ—¶å‘é‡å®Œå…¨æ··ä¹±ï¼šæ¯”å¦‚æ­£æ ·æœ¬ xâºå’Œé”šç‚¹ x çš„å¤¹è§’å¾ˆå¤§ï¼Œè´Ÿæ ·æœ¬ xâ»â‚å’Œ x çš„å¤¹è§’å¾ˆå° â€”â€” æŸå¤±å‡½æ•°ä¼šæ•æ‰åˆ°è¿™ç§ â€œè¯­ä¹‰ä¸åŒ¹é…â€ï¼Œå¹¶è§¦å‘ä¼˜åŒ–ã€‚
ä¸‰ã€Step2ï¼šè®¡ç®— InfoNCE Lossï¼ˆæ ¸å¿ƒå…¬å¼åº”ç”¨ï¼‰
æ ¹æ® InfoNCE Loss å…¬å¼ï¼Œåˆ† 3 æ­¥è®¡ç®—æŸå¤±ï¼š
1. è®¡ç®—æ‰€æœ‰æ ·æœ¬å¯¹çš„å¤¹è§’ä½™å¼¦å€¼ï¼ˆcosÎ¸ï¼‰
å…³é”®å…¬å¼ï¼šcosÎ¸â‚“áµ§ = (xãƒ»y)/(âˆ¥xâˆ¥ãƒ»âˆ¥yâˆ¥)ï¼ˆç‚¹ç§¯ Ã· é•¿åº¦ä¹˜ç§¯ï¼‰
æ­£æ ·æœ¬å¯¹ï¼ˆx, xâºï¼‰ï¼šxãƒ»xâº = 0.2Ã—0.5 + 0.3Ã—0.1 = 0.1 + 0.03 = 0.13cosÎ¸â‚“â‚“âº = 0.13/(0.36Ã—0.51) â‰ˆ 0.13/0.18 â‰ˆ 0.722ï¼ˆå¤¹è§’â‰ˆ43Â°ï¼Œå¤ªå¤§ï¼Œä¸ç¬¦åˆ â€œåŒä¹‰å¥è¿‘â€ï¼‰
è´Ÿæ ·æœ¬å¯¹ï¼ˆx, xâ»â‚ï¼‰ï¼šxãƒ»xâ»â‚ = 0.2Ã—0.7 + 0.3Ã—0.8 = 0.14 + 0.24 = 0.38cosÎ¸â‚“â‚“â»Â¹ = 0.38/(0.36Ã—1.06) â‰ˆ 0.38/0.38 â‰ˆ 1.0ï¼ˆå¤¹è§’â‰ˆ0Â°ï¼Œå¤ªå°ï¼Œä¸ç¬¦åˆ â€œéåŒä¹‰å¥è¿œâ€ï¼‰
è´Ÿæ ·æœ¬å¯¹ï¼ˆx, xâ»â‚‚ï¼‰ï¼šxãƒ»xâ»â‚‚ = 0.2Ã—0.1 + 0.3Ã—0.9 = 0.02 + 0.27 = 0.29cosÎ¸â‚“â‚“â»Â² = 0.29/(0.36Ã—0.91) â‰ˆ 0.29/0.33 â‰ˆ 0.879ï¼ˆå¤¹è§’â‰ˆ28Â°ï¼Œå¤ªå°ï¼Œä¸ç¬¦åˆçº¦æŸï¼‰
2. è®¡ç®—åˆ†å­å’Œåˆ†æ¯ï¼ˆå…¬å¼æ ¸å¿ƒé¡¹ï¼‰
åˆ†å­ï¼šexp (cosÎ¸â‚“â‚“âº / Ï„) = exp (0.722 / 0.1) = exp (7.22) â‰ˆ 1360åˆ†æ¯ï¼šåˆ†å­ + sum (exp (cosÎ¸â‚“â‚“â»áµ¢ / Ï„)) = 1360 + exp (1.0/0.1) + exp (0.879/0.1)= 1360 + exp(10) + exp(8.79) â‰ˆ 1360 + 22026 + 7350 â‰ˆ 30736
3. è®¡ç®—æœ€ç»ˆæŸå¤±
L = -log (åˆ†å­ / åˆ†æ¯) = -log (1360/30736) â‰ˆ -log (0.044) â‰ˆ 3.13ï¼ˆæŸå¤±å€¼å¾ˆå¤§ï¼Œè¯´æ˜å‘é‡ä¸¥é‡ä¸ç¬¦åˆè¯­ä¹‰çº¦æŸï¼‰
å››ã€Step3ï¼šåå‘ä¼ æ’­ä¼˜åŒ–ï¼ˆæŸå¤±å‡½æ•°é€¼ç€å‘é‡è°ƒæ•´ï¼‰
æ¨¡å‹çš„ç›®æ ‡æ˜¯ â€œæœ€å°åŒ–æŸå¤± Lâ€ï¼Œé€šè¿‡ åå‘ä¼ æ’­ è°ƒæ•´å‘é‡çš„æ¯ä¸ªåˆ†é‡ï¼ˆ0.2ã€0.3ã€0.5 ç­‰æ•°å€¼ï¼‰ï¼Œè°ƒæ•´æ–¹å‘å®Œå…¨ç”± InfoNCE Loss çš„æ¢¯åº¦å†³å®šï¼š
å¯¹æ­£æ ·æœ¬ xâºï¼šè¦è®© cosÎ¸â‚“â‚“âºå¢å¤§ï¼ˆæ¥è¿‘ 1ï¼‰â†’ è°ƒæ•´ xâºçš„åˆ†é‡ï¼Œè®©å®ƒå’Œ x çš„æ–¹å‘æ›´æ¥è¿‘ï¼ˆæ¯”å¦‚ xâºä» [0.5,0.1]â†’[0.3,0.4]ï¼Œå’Œ x=[0.2,0.3] æ–¹å‘è¶‹åŒï¼‰ï¼›
å¯¹è´Ÿæ ·æœ¬ xâ»â‚ã€xâ»â‚‚ï¼šè¦è®© cosÎ¸â‚“â‚“â»áµ¢å‡å°ï¼ˆæ¥è¿‘ - 1ï¼‰â†’ è°ƒæ•´ xâ»â‚ã€xâ»â‚‚çš„åˆ†é‡ï¼Œè®©å®ƒä»¬å’Œ x çš„æ–¹å‘ç›¸åï¼ˆæ¯”å¦‚ xâ»â‚ä» [0.7,0.8]â†’[-0.3,-0.4]ï¼Œå’Œ x æ–¹å‘ç›¸åï¼‰ï¼›
å¯¹é”šç‚¹ xï¼šå¾®è°ƒåˆ†é‡ï¼Œè®©å®ƒå’Œ xâºçš„æ–¹å‘æ›´ä¸€è‡´ï¼ŒåŒæ—¶å’Œ xâ»â‚ã€xâ»â‚‚çš„æ–¹å‘æ›´ç›¸åã€‚
è¿™ä¸ªè¿‡ç¨‹ä¼š åå¤è¿­ä»£ï¼ˆæ¯”å¦‚è®­ç»ƒ 1000 è½®ï¼‰ï¼Œæ¯ä¸€è½®éƒ½é‡æ–°è®¡ç®—æŸå¤±ã€è°ƒæ•´å‘é‡ï¼Œç›´åˆ°æŸå¤±é™åˆ°æœ€ä½ã€‚
äº”ã€Step4ï¼šè®­ç»ƒæ”¶æ•›ï¼ˆå‘é‡æ»¡è¶³è¯­ä¹‰çº¦æŸï¼‰
ç»è¿‡å¤šè½®è¿­ä»£åï¼ŒæŸå¤± L ä» 3.13 é™åˆ° 0.01ï¼ˆæ¥è¿‘æœ€å°å€¼ï¼‰ï¼Œæ­¤æ—¶çš„å‘é‡å®Œå…¨ç¬¦åˆè¯­ä¹‰é€»è¾‘ï¼š
é”šç‚¹å‘é‡ x = [3, 4]áµ€ï¼ˆé•¿åº¦âˆ¥xâˆ¥=5ï¼Œæ ¸å¿ƒè¯­ä¹‰ï¼šè‹¹æœï¼‰
æ­£æ ·æœ¬å‘é‡ xâº = [6, 8]áµ€ï¼ˆé•¿åº¦âˆ¥xâºâˆ¥=10ï¼Œæ˜¯ x çš„ 2 å€ï¼Œæ–¹å‘å®Œå…¨ç›¸åŒï¼‰
è´Ÿæ ·æœ¬å‘é‡ xâ»â‚ = [-3, -4]áµ€ï¼ˆé•¿åº¦âˆ¥xâ»â‚âˆ¥=5ï¼Œå’Œ x æ–¹å‘å®Œå…¨ç›¸åï¼‰
è´Ÿæ ·æœ¬å‘é‡ xâ»â‚‚ = [-6, -8]áµ€ï¼ˆé•¿åº¦âˆ¥xâ»â‚‚âˆ¥=10ï¼Œå’Œ x æ–¹å‘å®Œå…¨ç›¸åï¼‰
éªŒè¯ï¼šé‡æ–°è®¡ç®—æŸå¤±ï¼ˆç¬¦åˆçº¦æŸï¼‰
è®¡ç®— cosÎ¸ï¼š
cosÎ¸â‚“â‚“âº = (3Ã—6 + 4Ã—8)/(5Ã—10) = (18+32)/50 = 50/50 = 1.0ï¼ˆå¤¹è§’ 0Â°ï¼ŒåŒä¹‰å¥è¿‘ï¼‰
cosÎ¸â‚“â‚“â»Â¹ = (3Ã—(-3) + 4Ã—(-4))/(5Ã—5) = (-9-16)/25 = -25/25 = -1.0ï¼ˆå¤¹è§’ 180Â°ï¼ŒéåŒä¹‰å¥è¿œï¼‰
cosÎ¸â‚“â‚“â»Â² = (3Ã—(-6) + 4Ã—(-8))/(5Ã—10) = (-18-32)/50 = -50/50 = -1.0ï¼ˆå¤¹è§’ 180Â°ï¼Œç¬¦åˆçº¦æŸï¼‰
è®¡ç®—æŸå¤±ï¼šåˆ†å­ = exp (1.0/0.1) = exp (10) â‰ˆ 22026åˆ†æ¯ = 22026 + exp (-1.0/0.1) + exp (-1.0/0.1) = 22026 + 2Ã—exp (-10) â‰ˆ 22026ï¼ˆexp (-10)â‰ˆ4.5e-5ï¼Œå¯å¿½ç•¥ï¼‰L = -log (22026/22026) = -log (1) = 0ï¼ˆæŸå¤±æœ€å°ï¼Œå‘é‡å®Œå…¨æ»¡è¶³è¯­ä¹‰çº¦æŸï¼‰
å…­ã€è®­ç»ƒç»“æœçš„æ ¸å¿ƒæ„ä¹‰
å‘é‡å…³ç³»åŒ¹é…è¯­ä¹‰ï¼š
åŒä¹‰å¥ï¼ˆx å’Œ xâºï¼‰ï¼šæ–¹å‘ç›¸åŒï¼ˆå¤¹è§’ 0Â°ï¼‰ï¼Œé•¿åº¦ä¸åŒï¼ˆxâºæ›´é•¿ï¼Œä»£è¡¨è¯­ä¹‰å¼ºåº¦æ›´å¼ºï¼‰ï¼›
éåŒä¹‰å¥ï¼ˆx å’Œ xâ»â‚ã€xâ»â‚‚ï¼‰ï¼šæ–¹å‘ç›¸åï¼ˆå¤¹è§’ 180Â°ï¼‰ï¼Œé•¿åº¦ä¸å½±å“è¯­ä¹‰å·®å¼‚ã€‚
æŸå¤±å‡½æ•°çš„ä½œç”¨ï¼š
æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼ŒInfoNCE Loss æ˜¯ â€œæŒ‡æŒ¥æ£’â€â€”â€” é€šè¿‡ â€œæƒ©ç½šä¸ç¬¦åˆè¯­ä¹‰çš„å‘é‡å…³ç³»â€ï¼ˆåˆå§‹æŸå¤±å¤§ï¼‰ï¼Œé€¼ç€æ¨¡å‹è°ƒæ•´å‘é‡ï¼Œæœ€ç»ˆè®© â€œå‘é‡å¤¹è§’â€ å®Œç¾åŒ¹é… â€œè¯­ä¹‰ç›¸ä¼¼åº¦â€ã€‚
å®é™…åº”ç”¨ä»·å€¼ï¼š
è®­ç»ƒå¥½åï¼Œç»™æ¨¡å‹è¾“å…¥ â€œæˆ‘çˆ±åƒçº¢è‹¹æœâ€ï¼ˆæ–°çš„è‹¹æœç›¸å…³å¥å­ï¼‰ï¼Œå®ƒä¼šè¾“å‡ºå’Œ x æ–¹å‘æ¥è¿‘çš„å‘é‡ï¼ˆå¤¹è§’å°ï¼‰ï¼›è¾“å…¥ â€œç”µè„‘å¾ˆå¥½ç”¨â€ï¼ˆæ— å…³å¥å­ï¼‰ï¼Œä¼šè¾“å‡ºå’Œ x æ–¹å‘ç›¸åçš„å‘é‡ï¼ˆå¤¹è§’å¤§ï¼‰â€”â€” è¿™å°±æ˜¯ â€œå¤¹è§’ä»£è¡¨è¯­ä¹‰ç›¸ä¼¼åº¦â€ çš„æ¥æºã€‚
æ€»ç»“ï¼ˆå®é™…è®­ç»ƒçš„æ ¸å¿ƒé€»è¾‘ï¼‰
å¯¹æ¯”å­¦ä¹ ï¼ˆInfoNCE Lossï¼‰çš„å®é™…è®­ç»ƒï¼Œå°±æ˜¯ â€œéšæœºå‘é‡â†’è®¡ç®—æŸå¤±ï¼ˆæ•æ‰è¯­ä¹‰ä¸åŒ¹é…ï¼‰â†’åå‘ä¼ æ’­è°ƒæ•´å‘é‡â†’æŸå¤±æœ€å°ï¼ˆå‘é‡åŒ¹é…è¯­ä¹‰ï¼‰â€ çš„å¾ªç¯ã€‚æˆ‘ä»¬ä¸¾çš„ 2 ç»´å‘é‡ä¾‹å­ï¼Œå’Œå®é™…å¤§æ¨¡å‹ 768 ç»´å‘é‡çš„è®­ç»ƒé€»è¾‘å®Œå…¨ä¸€è‡´ â€”â€” æŸå¤±å‡½æ•°é€šè¿‡æ•°å­¦çº¦æŸï¼ŒæŠŠ â€œåŒä¹‰è¿‘ã€éåŒä¹‰è¿œâ€ çš„è¯­ä¹‰é€»è¾‘ï¼Œåˆ»è¿›äº†å‘é‡çš„å‡ ä½•å…³ç³»é‡Œã€‚
'''
def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), rope_base: float = 1e6,
                         rope_scaling: Optional[dict] = None):
    freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # # yarné•¿åº¦å¤–æ¨
    if rope_scaling is not None:
        orig_max, factor, beta_fast, beta_slow = (
            rope_scaling.get("original_max_position_embeddings", 2048), rope_scaling.get("factor", 4),
            rope_scaling.get("beta_fast", 4.0), rope_scaling.get("beta_slow", 1.0)
        )
        
        if end / orig_max > 1.0:
            corr_dim = next((i for i in range(dim // 2) if 2 * math.pi / freqs[i] > orig_max), dim // 2)
            power = torch.arange(0, dim // 2, device=freqs.device).float() / max(dim // 2 - 1, 1)
            beta = beta_slow + (beta_fast - beta_slow) * power
            # Î» = (Î²Â·Î± - Î² + 1)/(Î²Â·Î±) YaRNæ ‡å‡†å…¬å¼
            scale = torch.where(torch.arange(dim // 2, device=freqs.device) < corr_dim, (beta * factor - beta + 1) / (beta * factor), 1.0 / factor)
            freqs = freqs * scale

    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float()
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)
    return freqs_cos, freqs_sin


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    def rotate_half(x):
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        x[:, :, :, None, :].expand(bs, slen, num_key_value_heads, n_rep, head_dim).reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )


class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        assert args.num_attention_heads % self.num_key_value_heads == 0
        self.n_local_heads = args.num_attention_heads
        self.n_local_kv_heads = self.num_key_value_heads
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        self.head_dim = args.hidden_size // args.num_attention_heads
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
        # print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(self,
                x: torch.Tensor,
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # ä¿®æ”¹ä¸ºæ¥æ”¶coså’Œsin
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache=False,
                attention_mask: Optional[torch.Tensor] = None):
        bsz, seq_len, _ = x.shape
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)

        cos, sin = position_embeddings
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # kv_cacheå®ç°
        if past_key_value is not None:
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        past_kv = (xk, xv) if use_cache else None

        xq, xk, xv = (
            xq.transpose(1, 2),
            repeat_kv(xk, self.n_rep).transpose(1, 2),
            repeat_kv(xv, self.n_rep).transpose(1, 2)
        )

        if self.flash and seq_len > 1 and (attention_mask is None or torch.all(attention_mask == 1)):
            attn_mask = (
                None
                if attention_mask is None
                else attention_mask.view(bsz, 1, 1, -1).expand(bsz, self.n_local_heads, seq_len, -1).bool()
            )

            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=self.dropout if self.training else 0.0, is_causal=True)
        else:
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            scores = scores + torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1
            ).unsqueeze(0).unsqueeze(0)  # scores+mask

            if attention_mask is not None:
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask

            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            output = scores @ xv

        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        output = self.resid_dropout(self.o_proj(output))
        return output, past_kv


class FeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 / 3)
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.dropout = nn.Dropout(config.dropout)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))


class MoEGate(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.n_routed_experts = config.n_routed_experts

        self.scoring_func = config.scoring_func
        self.alpha = config.aux_loss_alpha
        self.seq_aux = config.seq_aux

        self.norm_topk_prob = config.norm_topk_prob
        self.gating_dim = config.hidden_size
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        self.reset_parameters()

    def reset_parameters(self) -> None:
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        bsz, seq_len, h = hidden_states.shape
        hidden_states = hidden_states.view(-1, h)
        logits = F.linear(hidden_states, self.weight, None)
        if self.scoring_func == 'softmax':
            scores = logits.softmax(dim=-1)
        else:
            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')

        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)

        if self.top_k > 1 and self.norm_topk_prob:
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            topk_weight = topk_weight / denominator

        if self.training and self.alpha > 0.0:
            scores_for_aux = scores
            aux_topk = self.top_k
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1)
            if self.seq_aux:
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                ce.scatter_add_(1, topk_idx_for_aux_loss,
                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(
                    seq_len * aux_topk / self.n_routed_experts)
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                ce = mask_ce.float().mean(0)
                Pi = scores_for_aux.mean(0)
                fi = ce * self.n_routed_experts
                aux_loss = (Pi * fi).sum() * self.alpha
        else:
            aux_loss = 0
        return topk_idx, topk_weight, aux_loss


class MOEFeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.experts = nn.ModuleList([
            FeedForward(config)
            for _ in range(config.n_routed_experts)
        ])
        self.gate = MoEGate(config)
        if config.n_shared_experts > 0:
            self.shared_experts = nn.ModuleList([
                FeedForward(config)
                for _ in range(config.n_shared_experts)
            ])

    def forward(self, x):
        identity = x
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        # ä½¿ç”¨é—¨æ§æœºåˆ¶é€‰æ‹©ä¸“å®¶
        topk_idx, topk_weight, aux_loss = self.gate(x)
        x = x.view(-1, x.shape[-1])
        flat_topk_idx = topk_idx.view(-1)
        if self.training:
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
            y = torch.empty_like(x, dtype=torch.float16)
            for i, expert in enumerate(self.experts):
                y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to(y.dtype)  # ç¡®ä¿ç±»å‹ä¸€è‡´
            y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1)
            y = y.view(*orig_shape)
        else:
            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity)
        self.aux_loss = aux_loss
        return y

    @torch.no_grad()
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        expert_cache = torch.zeros_like(x)
        idxs = flat_expert_indices.argsort()
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        token_idxs = idxs // self.config.num_experts_per_tok
        # å½“tokens_per_expert = [6, 15, 20, 26]ï¼Œtokens_per_expert.shape[0]å³ä¸ºä¸“å®¶æ•°é‡ï¼ˆæ­¤æ—¶ä¸º4ï¼‰
        # ä¸”token_idxs = [3, 7, 19, 21, 24, 25,  4,  5,  6, 10, 11, 12...] æ—¶
        # æ„å‘³token_idxs[:6] -> [3, 7, 19, 21, 24, 25]è¿™6ä¸ªä½ç½®å±äºä¸“å®¶0å¤„ç†çš„tokenï¼ˆæ¯ä¸ªtokenæœ‰å¯èƒ½è¢«å¤šä¸ªä¸“å®¶å¤„ç†ï¼Œè¿™å–å†³äºnum_experts_per_tokï¼‰
        # æ¥ä¸‹æ¥9ä¸ªä½ç½®token_idxs[6:15] -> [4,  5,  6, 10, 11, 12...]å±äºä¸“å®¶1å¤„ç†çš„token...ä¾æ­¤ç±»æ¨
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]
            if start_idx == end_idx:
                continue
            expert = self.experts[i]
            exp_token_idx = token_idxs[start_idx:end_idx]
            expert_tokens = x[exp_token_idx]
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]])
            expert_cache.scatter_add_(0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out)

        return expert_cache


class MiniMindBlock(nn.Module):
    def __init__(self, layer_id: int, config: MiniMindConfig):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        '''
        self.self_attn(
            å½’ä¸€åŒ–åçš„hidden_states, 
            position_embeddings,  # ä½ç½®ç¼–ç ï¼ˆè¡¥å……è¯çš„é¡ºåºä¿¡æ¯ï¼‰
            past_key_value, use_cache, attention_mask  # ç¼“å­˜/æ©ç ç›¸å…³
        )
        è®¡ç®— â€œè‡ªæ³¨æ„åŠ›â€ï¼Œè®©æ¯ä¸ªè¯èƒ½ â€œçœ‹åˆ°â€ åºåˆ—ä¸­å…¶ä»–ç›¸å…³è¯ï¼ˆæ¯”å¦‚é¢„æµ‹ â€œè‹¹æœâ€ æ—¶ï¼Œå…³æ³¨ â€œæˆ‘çˆ±åƒâ€ï¼‰ï¼Œæ•æ‰ä¸Šä¸‹æ–‡è¯­ä¹‰ä¾èµ–ã€‚
        hidden_statesï¼ˆç»è¿‡æ³¨æ„åŠ›åŠ æƒåçš„å‘é‡ï¼‰+ present_key_valueï¼ˆæ³¨æ„åŠ›çš„ K/V ç¼“å­˜ï¼Œç”¨äºè‡ªå›å½’ç”Ÿæˆæ—¶æé€Ÿï¼‰ã€‚
        '''
        self.self_attn = Attention(config)

        self.layer_id = layer_id
        # å¯¹ hidden_states åš Layer Normalizationï¼ˆå±‚å½’ä¸€åŒ–ï¼‰ï¼Œè®©å‘é‡çš„å‡å€¼æ¥è¿‘ 0ã€æ–¹å·®æ¥è¿‘ 1ã€‚
        # ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…è¾“å…¥å€¼è¿‡å¤§ / è¿‡å°å¯¼è‡´çš„æ¢¯åº¦çˆ†ç‚¸ / æ¶ˆå¤±ï¼Œè®©æ³¨æ„åŠ›æœºåˆ¶èƒ½æ›´é«˜æ•ˆåœ°å­¦ä¹ ä¸Šä¸‹æ–‡ä¾èµ–ã€‚
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        '''
        è®©æ¨¡å‹ â€œç›´æ¥ä¿ç•™åŸå§‹ç‰¹å¾â€ï¼ŒåŒæ—¶å åŠ æ³¨æ„åŠ›å­¦åˆ°çš„ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œé¿å…æ·±å±‚ç½‘ç»œä¸­ç‰¹å¾è¢«è¿‡åº¦æ‰­æ›²ï¼ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›ã€‚
        self.post_attention_layernorm(hidden_states)  # ç¬¬äºŒæ¬¡å½’ä¸€åŒ–
        self.mlp(...)  # å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰
        ä½œç”¨ï¼š
            å½’ä¸€åŒ–ï¼šå¯¹ â€œæ³¨æ„åŠ› + æ®‹å·®â€ åçš„å‘é‡å†åšä¸€æ¬¡ Layer Normalizationï¼Œç¨³å®š MLP çš„è¾“å…¥ï¼›
            MLP å¤„ç†ï¼šé€šè¿‡å…¨è¿æ¥å±‚ + æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ GELUï¼‰ï¼Œå¯¹æ¯ä¸ªè¯çš„å‘é‡åš â€œéçº¿æ€§ç‰¹å¾å¢å¼ºâ€ï¼ˆæ¯”å¦‚æŠŠ â€œæˆ‘â€ å’Œ â€œåƒâ€ çš„ç‰¹å¾ç»„åˆæˆ â€œæˆ‘åƒâ€ çš„è¯­ä¹‰ç‰¹å¾ï¼‰ã€‚
            ä¸ºä»€ä¹ˆï¼Ÿï¼šæ³¨æ„åŠ›å—è´Ÿè´£ â€œæ•æ‰ä¸Šä¸‹æ–‡å…³ç³»â€ï¼ŒMLP å—è´Ÿè´£ â€œå¼ºåŒ–å•ä¸ªè¯çš„è¯­ä¹‰ç‰¹å¾â€ï¼Œä¸¤è€…åˆ†å·¥åä½œã€‚
        '''
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    '''
    Pre-LN èŒƒå¼ä¸‹ Transformer è§£ç å™¨å±‚çš„æ ‡å‡†é¡ºåº
    
    æ•´ä½“é¡ºåºæ€»ç»“ï¼ˆä¸€å¥è¯ä¸²èµ·æ¥ï¼‰
    è¾“å…¥ â†’ ä¿å­˜æ®‹å·® 1 â†’ å½’ä¸€åŒ– 1 â†’ è‡ªæ³¨æ„åŠ›ï¼ˆæ•æ‰ä¸Šä¸‹æ–‡ï¼‰ â†’ æ®‹å·® 1 è¿æ¥ â†’ å½’ä¸€åŒ– 2 â†’ MLPï¼ˆå¢å¼ºç‰¹å¾ï¼‰ â†’ æ®‹å·® 2 è¿æ¥ â†’ è¾“å‡º
    å…³é”®æ³¨æ„ç‚¹ï¼ˆä¸ºä»€ä¹ˆé¡ºåºä¸èƒ½ä¹±ï¼Ÿï¼‰
    å½’ä¸€åŒ–çš„ä½ç½®ï¼ˆPre-LNï¼‰ï¼šå¿…é¡»åœ¨æ³¨æ„åŠ› / MLP ä¹‹å‰ï¼Œå¦åˆ™ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼ˆæ¢¯åº¦æ¶ˆå¤± / çˆ†ç‚¸ï¼‰ï¼Œè¿™æ˜¯ç°åœ¨å¤§æ¨¡å‹ï¼ˆå¦‚ GPTã€LLaMAï¼‰çš„æ ‡å‡†è®¾è®¡ï¼›
    æ®‹å·®è¿æ¥çš„æ—¶æœºï¼šæ¯æ¬¡ç»è¿‡æ³¨æ„åŠ› / MLP åï¼Œå¿…é¡»é©¬ä¸Šå’Œ â€œè¯¥æ¨¡å—çš„åŸå§‹è¾“å…¥â€ åšæ®‹å·®è¿æ¥ï¼Œå¦åˆ™æ— æ³•å‘æŒ¥æ®‹å·®çš„ä½œç”¨ï¼›
    æ³¨æ„åŠ›åœ¨å‰ï¼ŒMLP åœ¨åï¼šæ³¨æ„åŠ›è´Ÿè´£ â€œå…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–â€ï¼ŒMLP è´Ÿè´£ â€œå±€éƒ¨ç‰¹å¾éçº¿æ€§å¢å¼ºâ€ï¼Œå…ˆæ•æ‰å…³ç³»å†å¢å¼ºç‰¹å¾ï¼Œç¬¦åˆè¯­è¨€æ¨¡å‹çš„å­¦ä¹ é€»è¾‘ï¼›
    ä½ç½®ç¼–ç ï¼ˆposition_embeddingsï¼‰ï¼šä½œä¸ºæ³¨æ„åŠ›çš„è¾“å…¥ä¹‹ä¸€ï¼Œå¿…é¡»åœ¨æ³¨æ„åŠ›è®¡ç®—æ—¶åŠ å…¥ï¼ˆå¦åˆ™æ¨¡å‹ä¸çŸ¥é“è¯çš„é¡ºåºï¼‰ï¼Œä½†ä¸å‚ä¸å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥ï¼ˆå› ä¸ºä½ç½®ç¼–ç æ˜¯å›ºå®š / åŠå›ºå®šçš„ä½ç½®ä¿¡æ¯ï¼Œä¸éœ€è¦è¢«æ¨¡å‹ â€œä¿®æ­£â€ï¼‰ã€‚
    '''
    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        # æŠŠå½“å‰å±‚çš„åŸå§‹è¾“å…¥å­˜èµ·æ¥ï¼Œåé¢ç”¨äº â€œæ®‹å·®è¿æ¥â€ï¼ˆResidual Connectionï¼‰ã€‚
        residual = hidden_states
        hidden_states, present_key_value = self.self_attn(
            # ç¬¬ä¸€ä¸ªMiniMindBlockçš„hidden_statesæ˜¯éšæœºçš„embeddingå‘é‡
            self.input_layernorm(hidden_states), position_embeddings,
            past_key_value, use_cache, attention_mask
        )
        # æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ï¼šåŸå§‹è¾“å…¥ + æ³¨æ„åŠ›è¾“å‡º
        # è®©æ¨¡å‹ â€œç›´æ¥ä¿ç•™åŸå§‹ç‰¹å¾â€ï¼ŒåŒæ—¶å åŠ æ³¨æ„åŠ›å­¦åˆ°çš„ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œé¿å…æ·±å±‚ç½‘ç»œä¸­ç‰¹å¾è¢«è¿‡åº¦æ‰­æ›²ï¼ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›ã€‚
        hidden_states += residual
        '''
        ä½œç”¨ï¼šæŠŠ MLP å¢å¼ºåçš„ç‰¹å¾ï¼Œå’Œ â€œæ³¨æ„åŠ› + ç¬¬ä¸€æ¬¡æ®‹å·®â€ çš„ç‰¹å¾å åŠ ï¼Œä¿ç•™ä¸­é—´ç»“æœçš„åŒæ—¶ï¼Œæ³¨å…¥æ›´å¤æ‚çš„éçº¿æ€§ç‰¹å¾ã€‚
        æœ€ç»ˆè¾“å‡ºï¼šç»è¿‡ â€œæ³¨æ„åŠ›ä¸Šä¸‹æ–‡æ•æ‰ + MLP ç‰¹å¾å¢å¼ºâ€ çš„ hidden_statesï¼Œä»¥åŠç”¨äºç”ŸæˆåŠ é€Ÿçš„ present_key_valueã€‚

        residual2 = hidden_states  # ä¿å­˜æ³¨æ„åŠ›+ç¬¬ä¸€æ¬¡æ®‹å·®åçš„ç»“æœ
        mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))
        hidden_states = residual2 + mlp_output  # ç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥
        '''
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        return hidden_states, present_key_value


class MiniMindModel(nn.Module):
    '''
    è¿™æ˜¯ä¸€ä¸ª **MiniMind ç³»åˆ—çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausalLMï¼‰ç»“æ„**ï¼ˆç±»ä¼¼ GPTã€LLaMA ç­‰è‡ªå›å½’å¤§æ¨¡å‹ï¼‰ï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆæ¯”å¦‚ç»­å†™ã€é—®ç­”ï¼‰ã€‚ä¸‹é¢ä»ã€Œæ•´ä½“ç»“æ„ã€åˆ°ã€Œæ¯å±‚ç»†èŠ‚ã€ï¼Œé€éƒ¨åˆ†ç”¨é€šä¿—è¯­è¨€è§£é‡Šï¼Œå…¼é¡¾æŠ€æœ¯å‡†ç¡®æ€§å’Œæ˜“æ‡‚æ€§ï¼š

    ### å…ˆæ˜ç¡®æ ¸å¿ƒå®šä½
    - **MiniMindForCausalLM**ï¼šæœ€ç»ˆå¯¹å¤–æä¾›çš„ã€Œå› æœè¯­è¨€æ¨¡å‹ã€ç±»ï¼ˆCausalLM = å› æœè¯­è¨€æ¨¡å‹ï¼Œå³é€šè¿‡å‰æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œè‡ªå›å½’ç”Ÿæˆï¼‰ï¼›
    - æ ¸å¿ƒç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š`(model)` æ˜¯ã€Œç‰¹å¾æå– backboneï¼ˆMiniMindModelï¼‰ã€ï¼Œ`(lm_head)` æ˜¯ã€Œæœ€ç»ˆæ–‡æœ¬ç”Ÿæˆçš„è¾“å‡ºå¤´ã€ï¼›
    - æ‰€æœ‰å±‚çš„ `bias=False` è¡¨ç¤ºä¸ä½¿ç”¨åç½®é¡¹ï¼Œ`dropout(p=0.0)` è¡¨ç¤ºæœªå¯ç”¨ dropout æ­£åˆ™åŒ–ï¼ˆå¯èƒ½æ˜¯è®­ç»ƒåˆæœŸæˆ–æ¨ç†é˜¶æ®µï¼‰ã€‚


    ### æ•´ä½“ç»“æ„æ‹†è§£ï¼ˆä»å¤–åˆ°å†…ï¼‰
    ```
    MiniMindForCausalLM  # æ€»æ¨¡å‹ï¼ˆå› æœè¯­è¨€æ¨¡å‹ï¼‰
    â”œâ”€ (model): MiniMindModel  # æ ¸å¿ƒbackboneï¼ˆç‰¹å¾æå–å™¨ï¼‰
    â”‚  â”œâ”€ (embed_tokens): Embedding(6400, 512)  # TokenåµŒå…¥å±‚
    â”‚  â”œâ”€ (dropout): Dropout(p=0.0)  # åµŒå…¥å±‚åçš„dropoutï¼ˆæœªå¯ç”¨ï¼‰
    â”‚  â”œâ”€ (layers): ModuleList(8 x MiniMindBlock)  # 8å±‚Transformerç¼–ç å™¨å—ï¼ˆæ ¸å¿ƒè®¡ç®—å±‚ï¼‰
    â”‚  â”‚  â””â”€ æ¯å±‚ MiniMindBlockï¼ˆTransformerå—ï¼‰
    â”‚  â”‚     â”œâ”€ (self_attn): Attention  # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆè¿™é‡Œæ˜¯ä¼˜åŒ–åçš„ç¨€ç–/åˆ†ç»„æ³¨æ„åŠ›ï¼‰
    â”‚  â”‚     â”œâ”€ (input_layernorm): RMSNorm  # æ³¨æ„åŠ›å±‚å‰çš„å½’ä¸€åŒ–
    â”‚  â”‚     â”œâ”€ (post_attention_layernorm): RMSNorm  # æ³¨æ„åŠ›å±‚åçš„å½’ä¸€åŒ–
    â”‚  â”‚     â””â”€ (mlp): FeedForward  # å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆç‰¹å¾éçº¿æ€§å˜æ¢ï¼‰
    â”‚  â””â”€ (norm): RMSNorm  # æ‰€æœ‰Transformerå—è¾“å‡ºåçš„æœ€ç»ˆå½’ä¸€åŒ–
    â””â”€ (lm_head): Linear(512, 6400)  # è¯­è¨€æ¨¡å‹å¤´ï¼ˆå°†ç‰¹å¾æ˜ å°„ä¸ºtokenæ¦‚ç‡ï¼‰
    ```


    ### é€éƒ¨åˆ†è¯¦ç»†è§£é‡Š
    #### 1. æœ€å¤–å±‚ï¼šMiniMindForCausalLM
    - ä½œç”¨ï¼šPyTorch `nn.Module` çš„å­ç±»ï¼Œæ˜¯æ•´ä¸ªæ¨¡å‹çš„ã€Œå…¥å£ã€ï¼Œå°è£…äº† backbone å’Œè¾“å‡ºå¤´ï¼Œå¯¹å¤–æä¾› `forward()` æ–¹æ³•ï¼ˆæ¥æ”¶æ–‡æœ¬tokenï¼Œè¾“å‡ºä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡ï¼‰ï¼›
    - æ ¸å¿ƒé€»è¾‘ï¼š`æ–‡æœ¬token â†’ embed_tokensï¼ˆåµŒå…¥ï¼‰â†’ MiniMindBlockÃ—8ï¼ˆç‰¹å¾æå–ï¼‰â†’ normï¼ˆå½’ä¸€åŒ–ï¼‰â†’ lm_headï¼ˆé¢„æµ‹tokenï¼‰`ã€‚

    #### 2. (model): MiniMindModelï¼ˆæ ¸å¿ƒbackboneï¼‰
    - ä½œç”¨ï¼šå°†è¾“å…¥çš„ç¦»æ•£tokenï¼ˆæ¯”å¦‚æ•°å­—IDï¼‰è½¬æ¢ä¸ºè¿ç»­çš„ã€Œè¯­ä¹‰ç‰¹å¾å‘é‡ã€ï¼Œæ˜¯æ¨¡å‹çš„æ ¸å¿ƒè®¡ç®—éƒ¨åˆ†ï¼›
    - åŒ…å«ã€ŒåµŒå…¥å±‚ â†’ 8å±‚Transformerå— â†’ æœ€ç»ˆå½’ä¸€åŒ–ã€çš„å®Œæ•´ç‰¹å¾æå–æµç¨‹ã€‚

    ##### 2.1 (embed_tokens): Embedding(6400, 512)
    - ç±»å‹ï¼šè¯åµŒå…¥å±‚ï¼ˆPyTorch `nn.Embedding`ï¼‰ï¼›
    - å‚æ•°å«ä¹‰ï¼š`(vocab_size=6400, embed_dim=512)` â†’ è¯æ±‡è¡¨å¤§å°ä¸º 6400ï¼ˆæ¨¡å‹èƒ½è¯†åˆ« 6400 ä¸ªä¸åŒçš„tokenï¼‰ï¼Œæ¯ä¸ªtokenè¢«æ˜ å°„ä¸º 512 ç»´çš„ç¨ å¯†å‘é‡ï¼ˆåµŒå…¥å‘é‡ï¼‰ï¼›
    - ä½œç”¨ï¼šå°†ç¦»æ•£çš„token IDï¼ˆæ¯”å¦‚â€œä½ â€å¯¹åº” ID=123ï¼‰è½¬æ¢ä¸ºè¿ç»­çš„ã€æœ‰è¯­ä¹‰çš„å‘é‡ï¼ˆ512ç»´ï¼‰ï¼Œè®©æ¨¡å‹èƒ½ç†è§£tokençš„å«ä¹‰ï¼›
    - ä¸¾ä¾‹ï¼šè¾“å…¥token ID `[10, 25, 42]` â†’ è¾“å‡ºå½¢çŠ¶ `(batch_size, seq_len, 512)` çš„åµŒå…¥çŸ©é˜µã€‚

    ##### 2.2 (dropout): Dropout(p=0.0, inplace=False)
    - ä½œç”¨ï¼šåµŒå…¥å±‚åçš„æ­£åˆ™åŒ–å±‚ï¼Œéšæœºâ€œå…³é—­â€éƒ¨åˆ†ç¥ç»å…ƒï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼›
    - `p=0.0` è¡¨ç¤ºå½“å‰æœªå¯ç”¨ï¼ˆå¯èƒ½æ˜¯æ¨ç†é˜¶æ®µï¼Œæˆ–è®­ç»ƒåˆæœŸå…ˆä¸æ·»åŠ æ­£åˆ™åŒ–ï¼‰ï¼›
    - `inplace=False` è¡¨ç¤ºä¸ä¿®æ”¹è¾“å…¥å¼ é‡æœ¬èº«ï¼Œè€Œæ˜¯è¿”å›æ–°å¼ é‡ï¼ˆé¿å…è¦†ç›–åŸå§‹æ•°æ®ï¼‰ã€‚

    ##### 2.3 (layers): ModuleList(0-7): 8 x MiniMindBlock
    - ç±»å‹ï¼š`nn.ModuleList` æ˜¯PyTorchçš„â€œæ¨¡å—å®¹å™¨â€ï¼Œå­˜æ”¾ 8 ä¸ªç»“æ„ç›¸åŒçš„ `MiniMindBlock`ï¼ˆç±»ä¼¼Transformerçš„ç¼–ç å™¨å—ï¼‰ï¼›
    - ä½œç”¨ï¼š8å±‚å †å æ˜¯æ¨¡å‹â€œæ·±åº¦â€çš„ä½“ç°â€”â€”æ¯ä¸€å±‚éƒ½å¯¹åµŒå…¥ç‰¹å¾åšã€Œæ³¨æ„åŠ›äº¤äº’ + éçº¿æ€§å˜æ¢ã€ï¼Œå±‚å±‚æç‚¼æ›´å¤æ‚çš„è¯­ä¹‰ï¼ˆæ¯”å¦‚ä»å•ä¸ªtokenå«ä¹‰â†’çŸ­è¯­å«ä¹‰â†’å¥å­é€»è¾‘ï¼‰ï¼›
    - å…³é”®ï¼š8å±‚æ˜¯å¤§æ¨¡å‹çš„å¸¸è§â€œè½»é‡é…ç½®â€ï¼ˆæ¯”å¦‚ LLaMA-7B æ˜¯ 32 å±‚ï¼Œè¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆ MiniMindï¼‰ã€‚

    ###### 2.3.1 å•å±‚ MiniMindBlockï¼ˆTransformeræ ¸å¿ƒå—ï¼‰
    æ¯ä¸ªå—åŒ…å«ã€Œæ³¨æ„åŠ›å±‚ + ä¸¤ä¸ªå½’ä¸€åŒ–å±‚ + å‰é¦ˆç½‘ç»œã€ï¼Œæ˜¯å¤§æ¨¡å‹çš„â€œåŸºæœ¬è®¡ç®—å•å…ƒâ€ï¼Œé¡ºåºé€šå¸¸æ˜¯ï¼š`å½’ä¸€åŒ– â†’ æ³¨æ„åŠ› â†’ æ®‹å·®è¿æ¥ â†’ å½’ä¸€åŒ– â†’ å‰é¦ˆç½‘ç»œ â†’ æ®‹å·®è¿æ¥`ï¼ˆè¡Œä¸šä¸»æµçš„ Pre-LN ç»“æ„ï¼‰ã€‚

    ##### 2.3.1.1 (input_layernorm): RMSNorm()
    - ç±»å‹ï¼šRoot Mean Square Normalizationï¼ˆå‡æ–¹æ ¹å½’ä¸€åŒ–ï¼‰ï¼›
    - ä½œç”¨ï¼šåœ¨æ³¨æ„åŠ›å±‚ä¹‹å‰å¯¹è¾“å…¥ç‰¹å¾åšå½’ä¸€åŒ–ï¼Œè®©ç‰¹å¾çš„å‡å€¼â‰ˆ0ã€æ–¹å·®â‰ˆ1ï¼Œé¿å…æ¨¡å‹è®­ç»ƒæ—¶æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼›
    - ä¼˜åŠ¿ï¼šç›¸æ¯”ä¼ ç»Ÿçš„ LayerNormï¼ŒRMSNorm è®¡ç®—æ›´é«˜æ•ˆï¼ˆå°‘äº†å‡å€¼ä¸­å¿ƒåŒ–æ­¥éª¤ï¼‰ï¼Œæ˜¯å¤§æ¨¡å‹ä¸­å¸¸ç”¨çš„å½’ä¸€åŒ–æ–¹å¼ï¼ˆæ¯”å¦‚ LLaMAã€Qwen éƒ½ç”¨ RMSNormï¼‰ã€‚

    ##### 2.3.1.2 (self_attn): Attentionï¼ˆæ³¨æ„åŠ›å±‚ï¼‰
    - ä½œç”¨ï¼šè®©æ¯ä¸ªtokenèƒ½â€œå…³æ³¨â€è¾“å…¥åºåˆ—ä¸­å…¶ä»–ç›¸å…³tokençš„ä¿¡æ¯ï¼ˆæ¯”å¦‚â€œä»–å–œæ¬¢è‹¹æœâ€ä¸­ï¼Œâ€œä»–â€ä¼šå…³æ³¨â€œå–œæ¬¢â€å’Œâ€œè‹¹æœâ€ï¼‰ï¼Œæ•æ‰åºåˆ—çš„ä¾èµ–å…³ç³»ï¼ˆæ¯”å¦‚è¯­æ³•ã€è¯­ä¹‰å…³è”ï¼‰ï¼›
    - å†…éƒ¨ç»“æ„ï¼ˆä¼˜åŒ–ç‰ˆæ³¨æ„åŠ›ï¼Œéæ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼‰ï¼š
    - `(q_proj): Linear(in_features=512, out_features=512, bias=False)`ï¼šå°† 512 ç»´è¾“å…¥æ˜ å°„ä¸ºã€ŒæŸ¥è¯¢ï¼ˆQï¼‰ã€ï¼ˆ512ç»´ï¼‰ï¼›
    - `(k_proj): Linear(in_features=512, out_features=128, bias=False)`ï¼šå°† 512 ç»´è¾“å…¥æ˜ å°„ä¸ºã€Œé”®ï¼ˆKï¼‰ã€ï¼ˆ128ç»´ï¼‰ï¼›
    - `(v_proj): Linear(in_features=512, out_features=128, bias=False)`ï¼šå°† 512 ç»´è¾“å…¥æ˜ å°„ä¸ºã€Œå€¼ï¼ˆVï¼‰ã€ï¼ˆ128ç»´ï¼‰ï¼›
    - `(o_proj): Linear(in_features=512, out_features=512, bias=False)`ï¼šå°†æ³¨æ„åŠ›è®¡ç®—ç»“æœæ˜ å°„å› 512 ç»´ï¼ˆè¾“å‡ºæŠ•å½±ï¼‰ï¼›
    - å…³é”®ï¼šQ=512ç»´ï¼ŒK/V=128ç»´ â†’ è¿™æ˜¯ã€Œåˆ†ç»„æ³¨æ„åŠ›ã€æˆ–ã€Œç¨€ç–æ³¨æ„åŠ›ã€çš„ä¼˜åŒ–è®¾è®¡ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰ï¼Œè€Œéæ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼ˆQ/K/Vç»´åº¦é€šå¸¸ç›¸åŒï¼‰ï¼Œé€‚åˆè½»é‡çº§æ¨¡å‹ã€‚
    - è¾…åŠ©å±‚ï¼š
    - `(attn_dropout): Dropout(p=0.0)`ï¼šæ³¨æ„åŠ›æƒé‡çš„dropoutï¼ˆæœªå¯ç”¨ï¼‰ï¼›
    - `(resid_dropout): Dropout(p=0.0)`ï¼šæ³¨æ„åŠ›è¾“å‡ºçš„æ®‹å·®è¿æ¥å‰çš„dropoutï¼ˆæœªå¯ç”¨ï¼‰ã€‚

    ##### 2.3.1.3 (post_attention_layernorm): RMSNorm()
    - ä½œç”¨ï¼šæ³¨æ„åŠ›å±‚è¾“å‡ºåï¼Œå†åšä¸€æ¬¡å½’ä¸€åŒ–ï¼Œä¸ºåç»­å‰é¦ˆç½‘ç»œçš„éçº¿æ€§å˜æ¢åšå‡†å¤‡ï¼ˆPre-LN ç»“æ„çš„æ ‡å‡†æ­¥éª¤ï¼‰ã€‚

    ##### 2.3.1.4 (mlp): FeedForwardï¼ˆå‰é¦ˆç¥ç»ç½‘ç»œï¼‰
    - ä½œç”¨ï¼šå¯¹æ³¨æ„åŠ›å±‚è¾“å‡ºçš„ç‰¹å¾åšã€Œéçº¿æ€§å˜æ¢ã€ï¼Œæç‚¼æ›´å¤æ‚çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆæ³¨æ„åŠ›è´Ÿè´£â€œæ•æ‰ä¾èµ–â€ï¼ŒMLPè´Ÿè´£â€œå¼ºåŒ–ç‰¹å¾â€ï¼‰ï¼›
    - å†…éƒ¨ç»“æ„ï¼ˆGated MLPï¼Œé—¨æ§å‰é¦ˆç½‘ç»œï¼Œæ¯”æ™®é€šMLPæ›´é«˜æ•ˆï¼‰ï¼š
    - `(gate_proj): Linear(512, 1408, bias=False)`ï¼šé—¨æ§æŠ•å½±å±‚â€”â€”å°† 512 ç»´ç‰¹å¾æ˜ å°„ä¸º 1408 ç»´ï¼Œæ§åˆ¶ä¿¡æ¯ä¼ é€’ï¼ˆç±»ä¼¼â€œå¼€å…³â€ï¼‰ï¼›
    - `(up_proj): Linear(512, 1408, bias=False)`ï¼šä¸Šé‡‡æ ·æŠ•å½±å±‚â€”â€”åŒæ ·å°† 512 ç»´æ˜ å°„ä¸º 1408 ç»´ï¼ˆä¸ gate_proj è¾“å‡ºåšå…ƒç´ ä¹˜ï¼Œå®ç°é—¨æ§ï¼‰ï¼›
    - `(act_fn): SiLUActivation()`ï¼šæ¿€æ´»å‡½æ•°ï¼ˆSigmoid Linear Unitï¼‰â€”â€”å¼•å…¥éçº¿æ€§ï¼Œè®©æ¨¡å‹èƒ½å­¦ä¹ å¤æ‚å…³ç³»ï¼ˆæ¯”ReLUæ›´é€‚åˆå¤§æ¨¡å‹ï¼‰ï¼›
    - `(down_proj): Linear(1408, 512, bias=False)`ï¼šä¸‹é‡‡æ ·æŠ•å½±å±‚â€”â€”å°† 1408 ç»´ç‰¹å¾æ˜ å°„å› 512 ç»´ï¼ˆä¸è¾“å…¥ç»´åº¦ä¸€è‡´ï¼Œæ–¹ä¾¿æ®‹å·®è¿æ¥ï¼‰ï¼›
    - `(dropout): Dropout(p=0.0)`ï¼šMLPè¾“å‡ºçš„dropoutï¼ˆæœªå¯ç”¨ï¼‰ï¼›
    - å…³é”®ï¼š1408 æ˜¯éšè—å±‚ç»´åº¦ï¼Œé€šå¸¸æ˜¯è¾“å…¥ç»´åº¦ 512 çš„ 2.75 å€ï¼ˆ512Ã—2.75=1408ï¼‰ï¼Œæ˜¯å¤§æ¨¡å‹ä¸­å¸¸ç”¨çš„éšè—å±‚æ¯”ä¾‹ã€‚

    ##### 2.4 (norm): RMSNorm()
    - ä½œç”¨ï¼š8å±‚ MiniMindBlock å †å åï¼Œå¯¹æœ€ç»ˆçš„ç‰¹å¾åšä¸€æ¬¡å…¨å±€å½’ä¸€åŒ–ï¼Œç¡®ä¿ç‰¹å¾åˆ†å¸ƒç¨³å®šï¼Œä¸ºåç»­è¾“å‡ºå¤´åšå‡†å¤‡ã€‚

    #### 3. (lm_head): Linear(in_features=512, out_features=6400, bias=False)
    - ç±»å‹ï¼šçº¿æ€§æŠ•å½±å±‚ï¼ˆè¯­è¨€æ¨¡å‹å¤´ï¼‰ï¼›
    - å‚æ•°å«ä¹‰ï¼š`(in_features=512, out_features=6400)` â†’ æ¥æ”¶ 512 ç»´çš„è¯­ä¹‰ç‰¹å¾ï¼Œæ˜ å°„ä¸º 6400 ç»´çš„è¾“å‡ºï¼ˆä¸è¯æ±‡è¡¨å¤§å°ä¸€è‡´ï¼‰ï¼›
    - ä½œç”¨ï¼šå°† backbone æå–çš„è¯­ä¹‰ç‰¹å¾ï¼Œè½¬æ¢ä¸ºã€Œæ¯ä¸ªtokenåœ¨è¯æ±‡è¡¨ä¸­çš„æ¦‚ç‡åˆ†æ•°ã€â€”â€”è¾“å‡ºå½¢çŠ¶ä¸º `(batch_size, seq_len, 6400)`ï¼Œæ¯ä¸ªä½ç½®çš„ 6400 ä¸ªæ•°å€¼å¯¹åº”â€œè¯¥ä½ç½®æ˜¯è¯æ±‡è¡¨ä¸­ç¬¬ i ä¸ªtokenâ€çš„å¾—åˆ†ï¼›
    - åç»­æ­¥éª¤ï¼šè¾“å‡ºåä¼šç»è¿‡ `softmax` å‡½æ•°ï¼Œå°†å¾—åˆ†è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œæ¨¡å‹é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenä½œä¸ºâ€œä¸‹ä¸€ä¸ªè¦ç”Ÿæˆçš„tokenâ€ï¼ˆè‡ªå›å½’ç”Ÿæˆçš„æ ¸å¿ƒï¼‰ã€‚


    ### æ ¸å¿ƒå‚æ•°æ€»ç»“ï¼ˆå¿«é€ŸæŒæ¡æ¨¡å‹è§„æ¨¡ï¼‰
    | å‚æ•°                | æ•°å€¼       | å«ä¹‰                                  |
    |---------------------|------------|---------------------------------------|
    | è¯æ±‡è¡¨å¤§å°          | 6400       | æ¨¡å‹èƒ½è¯†åˆ«/ç”Ÿæˆçš„ä¸åŒtokenæ•°é‡        |
    | åµŒå…¥ç»´åº¦/æ¨¡å‹ç»´åº¦   | 512        | æ¯ä¸ªtokençš„ç‰¹å¾å‘é‡ç»´åº¦ï¼ˆæ¨¡å‹æ ¸å¿ƒç»´åº¦ï¼‰|
    | Transformerå—æ•°é‡   | 8å±‚        | æ¨¡å‹çš„æ·±åº¦ï¼ˆå±‚æ•°è¶Šå¤šï¼Œå»ºæ¨¡èƒ½åŠ›è¶Šå¼ºï¼‰  |
    | MLPéšè—å±‚ç»´åº¦       | 1408       | å‰é¦ˆç½‘ç»œçš„ä¸­é—´ç»´åº¦ï¼ˆ512Ã—2.75ï¼‰        |
    | æ³¨æ„åŠ›å±‚è®¾è®¡        | Q=512, K/V=128 | ä¼˜åŒ–ç‰ˆåˆ†ç»„æ³¨æ„åŠ›ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰        |
    | å½’ä¸€åŒ–æ–¹å¼          | RMSNorm    | é«˜æ•ˆç¨³å®šçš„å½’ä¸€åŒ–ï¼ˆå¤§æ¨¡å‹å¸¸ç”¨ï¼‰        |
    | æ¿€æ´»å‡½æ•°            | SiLU       | éçº¿æ€§æ¿€æ´»ï¼ˆé€‚åˆå¤§æ¨¡å‹ï¼‰              |


    ### æ¨¡å‹å·¥ä½œæµç¨‹ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ï¼‰
    è¾“å…¥æ–‡æœ¬ â†’ è½¬æ¢ä¸ºtoken ID â†’ `embed_tokens` æ˜ å°„ä¸º 512 ç»´åµŒå…¥å‘é‡ â†’ ç»è¿‡ 8 å±‚ `MiniMindBlock`ï¼ˆæ³¨æ„åŠ›æ•æ‰ä¾èµ– + MLPå¼ºåŒ–ç‰¹å¾ï¼‰â†’ æœ€ç»ˆ `RMSNorm` å½’ä¸€åŒ– â†’ `lm_head` æŠ•å½±ä¸º 6400 ä¸ªtokençš„æ¦‚ç‡ â†’ é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenä½œä¸ºä¸‹ä¸€ä¸ªç”Ÿæˆçš„tokenï¼Œé‡å¤è¯¥è¿‡ç¨‹å®ç°æ–‡æœ¬ç»­å†™ã€‚

    è¿™ä¸ªæ¨¡å‹æ˜¯ã€Œè½»é‡çº§å¤§æ¨¡å‹ã€ï¼ˆ512ç»´+8å±‚ï¼‰ï¼Œé€‚åˆå…¥é—¨å­¦ä¹ ã€å°æ•°æ®é›†è®­ç»ƒæˆ–è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼Œæ ¸å¿ƒç»“æ„å’Œ LLaMAã€Qwen ç­‰ä¸»æµå¤§æ¨¡å‹ä¸€è‡´ï¼Œåªæ˜¯è§„æ¨¡æ›´å°ã€è®¡ç®—é‡æ›´ä½ã€‚
    '''
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers
        # config.vocab_size å‘Šè¯‰ embedding å±‚ â€œæ€»å…±æœ‰å¤šå°‘ä¸ªä¸åŒçš„è¯éœ€è¦æ˜ å°„â€ã€‚ä¸ºæ¯ä¸ª token åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„å‘é‡
        # config.hidden_size è¿™ä¸ªç»´åº¦å†³å®šäº†å‘é‡èƒ½ â€œæ‰¿è½½å¤šå°‘è¯­ä¹‰ä¿¡æ¯â€ï¼šç»´åº¦è¶Šé«˜ï¼ˆå¦‚ 1024ï¼‰ï¼Œç†è®ºä¸Šèƒ½åŒºåˆ†çš„è¯­ä¹‰è¶Šç²¾ç»†ï¼Œä½†æ¨¡å‹å‚æ•°ä¹Ÿä¼šè¶Šå¤šï¼ˆè®¡ç®—æˆæœ¬è¶Šé«˜ï¼‰ã€‚
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.dropout = nn.Dropout(config.dropout)
        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        freqs_cos, freqs_sin = precompute_freqs_cis(dim=config.hidden_size // config.num_attention_heads,
                                                    end=config.max_position_embeddings, rope_base=config.rope_theta,
                                                    rope_scaling=config.rope_scaling)
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                **kwargs):
        batch_size, seq_length = input_ids.shape
        if hasattr(past_key_values, 'layers'): past_key_values = None
        past_key_values = past_key_values or [None] * len(self.layers)
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        hidden_states = self.dropout(self.embed_tokens(input_ids))

        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        presents = []
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)

        hidden_states = self.norm(hidden_states)

        aux_loss = sum(
            layer.mlp.aux_loss
            for layer in self.layers
            if isinstance(layer.mlp, MOEFeedForward)
        )

        return hidden_states, presents, aux_loss


class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    config_class = MiniMindConfig

    def __init__(self, config: MiniMindConfig = None):
        self.config = config or MiniMindConfig()
        super().__init__(self.config)
        self.model = MiniMindModel(self.config)
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        self.model.embed_tokens.weight = self.lm_head.weight
        self.OUT = CausalLMOutputWithPast()

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **args):
        h, past_kvs, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(h[:, slice_indices, :])
        self.OUT.__setitem__('last_hidden_state', h)
        self.OUT.__setitem__('logits', logits)
        self.OUT.__setitem__('aux_loss', aux_loss)
        self.OUT.__setitem__('past_key_values', past_kvs)
        return self.OUT
